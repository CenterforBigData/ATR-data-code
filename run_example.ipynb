{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ea93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3288dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from an Excel file into a pandas DataFrame.\n",
    "# The path provided should be updated to where the actual file is located.\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('Hawaii.xlsx')\n",
    "\n",
    "# Converting the 'year' and 'day' columns to string data types for consistency\n",
    "# and to facilitate any operations that require string manipulation.\n",
    "data[\"year\"] = data[\"year\"].astype(str)\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "\n",
    "# Ensuring that the 'Holiday' column is of type string. This is particularly useful\n",
    "# if the column contains textual data that represents holiday names or boolean values ('True', 'False').\n",
    "data['Holiday'] = data['Holiday'].astype(str)\n",
    "\n",
    "# The 'tourist', 'Trend', 'Seasonal', and 'Resid' columns are converted to floating point numbers.\n",
    "# This is essential for any subsequent mathematical operations and statistical analysis.\n",
    "# The 'tourist' column represents the number of tourists,\n",
    "# whereas 'Trend', 'Seasonal', and 'Resid' are components derived from a decomposition technique\n",
    "# such as RobustSTL, which are used for time series forecasting.\n",
    "data[\"tourist\"] = data[\"tourist\"].astype(\"float64\")\n",
    "data[\"Trend\"] = data[\"Trend\"].astype(\"float64\")\n",
    "data[\"Seasonal\"] = data[\"Seasonal\"].astype(\"float64\")\n",
    "data[\"Resid\"] = data[\"Resid\"].astype(\"float64\")\n",
    "\n",
    "# At this point, the 'data' DataFrame is ready for further analysis or preprocessing steps,\n",
    "# such as normalization, scaling, or additional feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 4481],  # Filter data up to a certain time index for training\n",
    "    time_idx=\"time_idx\",  # The name of the column that provides the time index\n",
    "    target=\"Trend\",  # The target column to predict (one of the components from the RobustSTL decomposition)\n",
    "    min_encoder_length=max_encoder_length // 2,  # Minimum length of the encoder, half of the max_encoder_length\n",
    "    max_encoder_length=max_encoder_length,  # Maximum length of historical data used for predictions\n",
    "    min_prediction_length=1,  # Minimum prediction length\n",
    "    max_prediction_length=max_prediction_length,  # Maximum prediction length\n",
    "    time_varying_known_categoricals=[\"month\", \"day of the week\", \"day\", \"Holiday\"],  # Known categorical features\n",
    "    time_varying_known_reals=[\"time_idx\"],  # Known real features, like time index\n",
    "    time_varying_unknown_categoricals=[],  # Unknown categorical features, if any\n",
    "    time_varying_unknown_reals=[\"Trend\"],  # Unknown real features, which includes the target 'Trend' itself\n",
    "    group_ids=['destination'],  # The column that identifies the time series group\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"  # A normalization technique\n",
    "    ),\n",
    "    add_relative_time_idx=True,  # Adds a column for relative time index\n",
    "    add_target_scales=True,  # Adds columns for target scales\n",
    "    add_encoder_length=True,  # Adds a column for encoder length\n",
    "    allow_missing_timesteps=True,  # Allows model to handle missing timesteps if there are any\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb69a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is dedicated to hyperparameter optimization using the Optuna framework\n",
    "# integrated within the pytorch-forecasting package. This step is optional due to its\n",
    "# time-consuming nature, but it is crucial for fine-tuning the model to achieve better performance.\n",
    "\n",
    "\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "# Initialize the hyperparameter optimization study which will search for the best hyperparameters\n",
    "# over a specified number of trials.\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,  # DataLoader containing the training data\n",
    "    val_dataloader,  # DataLoader containing the validation data\n",
    "    model_path=\"optuna_test\",  # Directory where the models are saved during optimization\n",
    "    n_trials=50,  # Number of trials to run\n",
    "    max_epochs=50,  # Maximum number of epochs to train the model for each trial\n",
    "    gradient_clip_val_range=(0.01, 1.0),  # Range for gradient clipping for avoiding exploding gradients\n",
    "    hidden_size_range=(8, 128),  # Range for the size of hidden layers\n",
    "    hidden_continuous_size_range=(8, 128),  # Range for the size of hidden continuous layers\n",
    "    attention_head_size_range=(1, 4),  # Range for the number of attention heads\n",
    "    learning_rate_range=(0.001, 0.1),  # Range for the learning rate\n",
    "    dropout_range=(0.1, 0.3),  # Range for dropout rates to prevent overfitting\n",
    "    trainer_kwargs=dict(limit_train_batches=30),  # Limit the number of batches for training to speed up epochs\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate when a plateau is reached\n",
    "    use_learning_rate_finder=False  # Whether to use the learning rate finder (here it is turned off)\n",
    ")\n",
    "\n",
    "# Save the results of the study to a file so that we can resume the optimization later if needed.\n",
    "# This is useful for long-running optimizations that may need to be stopped and restarted.\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# After the optimization study is complete, print out the best hyperparameters found.\n",
    "# These parameters can be used to configure the model for the final training.\n",
    "print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1c651",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679ff9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 858.9k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.03911626926390909,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.00842077448532244,  # Learning rate of the model\n",
    "    hidden_size=125,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.15160823136480017,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=17,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "232c0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 539   \n",
      "3  | prescalers                         | ModuleDict                      | 204   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 17.5 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.5 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 14.3 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 63.2 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 63.2 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 63.2 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 63.2 K\n",
      "11 | lstm_encoder                       | LSTM                            | 126 K \n",
      "12 | lstm_decoder                       | LSTM                            | 126 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 31.5 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 250   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 78.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 62.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 31.8 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 63.2 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 31.8 K\n",
      "20 | output_layer                       | Linear                          | 882   \n",
      "----------------------------------------------------------------------------------------\n",
      "858 K     Trainable params\n",
      "0         Non-trainable params\n",
      "858 K     Total params\n",
      "3.436     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|███████████████████████▏| 30/31 [00:10<00:00,  2.98it/s, loss=1.52e+03, v_num=4, train_loss_step=1.16e+3]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|████████| 31/31 [00:10<00:00,  2.84it/s, loss=1.52e+03, v_num=4, train_loss_step=1.16e+3, val_loss=871.0]\u001b[A\n",
      "Epoch 1:  97%|▉| 30/31 [00:10<00:00,  2.80it/s, loss=927, v_num=4, train_loss_step=578.0, val_loss=871.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 31/31 [00:11<00:00,  2.66it/s, loss=927, v_num=4, train_loss_step=578.0, val_loss=374.0, train_loss_ep\u001b[A\n",
      "Epoch 2:  97%|▉| 30/31 [00:11<00:00,  2.55it/s, loss=691, v_num=4, train_loss_step=652.0, val_loss=374.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 31/31 [00:12<00:00,  2.42it/s, loss=691, v_num=4, train_loss_step=652.0, val_loss=274.0, train_loss_ep\u001b[A\n",
      "Epoch 3:  97%|▉| 30/31 [00:16<00:00,  1.87it/s, loss=600, v_num=4, train_loss_step=634.0, val_loss=274.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 31/31 [00:17<00:00,  1.82it/s, loss=600, v_num=4, train_loss_step=634.0, val_loss=297.0, train_loss_ep\u001b[A\n",
      "Epoch 4:  97%|▉| 30/31 [00:11<00:00,  2.64it/s, loss=505, v_num=4, train_loss_step=545.0, val_loss=297.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 31/31 [00:12<00:00,  2.48it/s, loss=505, v_num=4, train_loss_step=545.0, val_loss=248.0, train_loss_ep\u001b[A\n",
      "Epoch 5:  97%|▉| 30/31 [00:11<00:00,  2.66it/s, loss=498, v_num=4, train_loss_step=537.0, val_loss=248.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=498, v_num=4, train_loss_step=537.0, val_loss=192.0, train_loss_ep\u001b[A\n",
      "Epoch 6:  97%|▉| 30/31 [00:11<00:00,  2.59it/s, loss=502, v_num=4, train_loss_step=555.0, val_loss=192.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 31/31 [00:12<00:00,  2.49it/s, loss=502, v_num=4, train_loss_step=555.0, val_loss=188.0, train_loss_ep\u001b[A\n",
      "Epoch 7:  97%|▉| 30/31 [00:11<00:00,  2.60it/s, loss=468, v_num=4, train_loss_step=414.0, val_loss=188.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 31/31 [00:12<00:00,  2.48it/s, loss=468, v_num=4, train_loss_step=414.0, val_loss=189.0, train_loss_ep\u001b[A\n",
      "Epoch 8:  97%|▉| 30/31 [00:11<00:00,  2.58it/s, loss=457, v_num=4, train_loss_step=545.0, val_loss=189.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 31/31 [00:12<00:00,  2.44it/s, loss=457, v_num=4, train_loss_step=545.0, val_loss=212.0, train_loss_ep\u001b[A\n",
      "Epoch 9:  97%|▉| 30/31 [00:11<00:00,  2.58it/s, loss=472, v_num=4, train_loss_step=534.0, val_loss=212.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 31/31 [00:12<00:00,  2.39it/s, loss=472, v_num=4, train_loss_step=534.0, val_loss=267.0, train_loss_ep\u001b[A\n",
      "Epoch 10:  97%|▉| 30/31 [00:10<00:00,  2.85it/s, loss=458, v_num=4, train_loss_step=488.0, val_loss=267.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 31/31 [00:11<00:00,  2.70it/s, loss=458, v_num=4, train_loss_step=488.0, val_loss=225.0, train_loss_e\u001b[A\n",
      "Epoch 11:  97%|▉| 30/31 [00:10<00:00,  2.75it/s, loss=456, v_num=4, train_loss_step=391.0, val_loss=225.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 31/31 [00:11<00:00,  2.60it/s, loss=456, v_num=4, train_loss_step=391.0, val_loss=162.0, train_loss_e\u001b[A\n",
      "Epoch 12:  97%|▉| 30/31 [00:11<00:00,  2.72it/s, loss=441, v_num=4, train_loss_step=408.0, val_loss=162.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 31/31 [00:12<00:00,  2.58it/s, loss=441, v_num=4, train_loss_step=408.0, val_loss=215.0, train_loss_e\u001b[A\n",
      "Epoch 13:  97%|▉| 30/31 [00:10<00:00,  2.80it/s, loss=409, v_num=4, train_loss_step=545.0, val_loss=215.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 31/31 [00:11<00:00,  2.63it/s, loss=409, v_num=4, train_loss_step=545.0, val_loss=208.0, train_loss_e\u001b[A\n",
      "Epoch 14:  97%|▉| 30/31 [00:10<00:00,  2.82it/s, loss=433, v_num=4, train_loss_step=377.0, val_loss=208.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 31/31 [00:11<00:00,  2.68it/s, loss=433, v_num=4, train_loss_step=377.0, val_loss=234.0, train_loss_e\u001b[A\n",
      "Epoch 15:  97%|▉| 30/31 [00:12<00:00,  2.46it/s, loss=397, v_num=4, train_loss_step=378.0, val_loss=234.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 31/31 [00:13<00:00,  2.35it/s, loss=397, v_num=4, train_loss_step=378.0, val_loss=191.0, train_loss_e\u001b[A\n",
      "Epoch 16:  97%|▉| 30/31 [00:11<00:00,  2.57it/s, loss=406, v_num=4, train_loss_step=452.0, val_loss=191.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 31/31 [00:12<00:00,  2.45it/s, loss=406, v_num=4, train_loss_step=452.0, val_loss=183.0, train_loss_e\u001b[A\n",
      "Epoch 17:  97%|▉| 30/31 [00:12<00:00,  2.46it/s, loss=386, v_num=4, train_loss_step=304.0, val_loss=183.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 31/31 [00:13<00:00,  2.35it/s, loss=386, v_num=4, train_loss_step=304.0, val_loss=178.0, train_loss_e\u001b[A\n",
      "Epoch 18:  97%|▉| 30/31 [00:12<00:00,  2.45it/s, loss=387, v_num=4, train_loss_step=320.0, val_loss=178.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 31/31 [00:13<00:00,  2.35it/s, loss=387, v_num=4, train_loss_step=320.0, val_loss=180.0, train_loss_e\u001b[A\n",
      "Epoch 19:  97%|▉| 30/31 [00:12<00:00,  2.47it/s, loss=386, v_num=4, train_loss_step=262.0, val_loss=180.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 31/31 [00:13<00:00,  2.37it/s, loss=386, v_num=4, train_loss_step=262.0, val_loss=127.0, train_loss_e\u001b[A\n",
      "Epoch 20:  97%|▉| 30/31 [00:10<00:00,  2.83it/s, loss=371, v_num=4, train_loss_step=333.0, val_loss=127.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 31/31 [00:11<00:00,  2.70it/s, loss=371, v_num=4, train_loss_step=333.0, val_loss=129.0, train_loss_e\u001b[A\n",
      "Epoch 21:  97%|▉| 30/31 [00:10<00:00,  2.91it/s, loss=380, v_num=4, train_loss_step=427.0, val_loss=129.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 31/31 [00:11<00:00,  2.77it/s, loss=380, v_num=4, train_loss_step=427.0, val_loss=128.0, train_loss_e\u001b[A\n",
      "Epoch 22:  97%|▉| 30/31 [00:10<00:00,  2.90it/s, loss=366, v_num=4, train_loss_step=277.0, val_loss=128.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 31/31 [00:11<00:00,  2.75it/s, loss=366, v_num=4, train_loss_step=277.0, val_loss=129.0, train_loss_e\u001b[A\n",
      "Epoch 23:  97%|▉| 30/31 [00:10<00:00,  2.73it/s, loss=402, v_num=4, train_loss_step=446.0, val_loss=129.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 31/31 [00:11<00:00,  2.59it/s, loss=402, v_num=4, train_loss_step=446.0, val_loss=176.0, train_loss_e\u001b[A\n",
      "Epoch 24:  97%|▉| 30/31 [00:11<00:00,  2.57it/s, loss=389, v_num=4, train_loss_step=380.0, val_loss=176.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 31/31 [00:12<00:00,  2.46it/s, loss=389, v_num=4, train_loss_step=380.0, val_loss=124.0, train_loss_e\u001b[A\n",
      "Epoch 25:  97%|▉| 30/31 [00:15<00:00,  1.96it/s, loss=387, v_num=4, train_loss_step=357.0, val_loss=124.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 31/31 [00:16<00:00,  1.91it/s, loss=387, v_num=4, train_loss_step=357.0, val_loss=120.0, train_loss_e\u001b[A\n",
      "Epoch 26:  97%|▉| 30/31 [00:10<00:00,  2.79it/s, loss=369, v_num=4, train_loss_step=346.0, val_loss=120.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 31/31 [00:11<00:00,  2.64it/s, loss=369, v_num=4, train_loss_step=346.0, val_loss=133.0, train_loss_e\u001b[A\n",
      "Epoch 27:  97%|▉| 30/31 [00:13<00:00,  2.30it/s, loss=390, v_num=4, train_loss_step=386.0, val_loss=133.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 31/31 [00:14<00:00,  2.20it/s, loss=390, v_num=4, train_loss_step=386.0, val_loss=173.0, train_loss_e\u001b[A\n",
      "Epoch 28:  97%|▉| 30/31 [00:12<00:00,  2.35it/s, loss=386, v_num=4, train_loss_step=359.0, val_loss=173.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 31/31 [00:13<00:00,  2.25it/s, loss=386, v_num=4, train_loss_step=359.0, val_loss=190.0, train_loss_e\u001b[A\n",
      "Epoch 29:  97%|▉| 30/31 [00:13<00:00,  2.29it/s, loss=388, v_num=4, train_loss_step=370.0, val_loss=190.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 31/31 [00:14<00:00,  2.20it/s, loss=388, v_num=4, train_loss_step=370.0, val_loss=192.0, train_loss_e\u001b[A\n",
      "Epoch 30:  97%|▉| 30/31 [00:12<00:00,  2.34it/s, loss=380, v_num=4, train_loss_step=274.0, val_loss=192.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 31/31 [00:13<00:00,  2.25it/s, loss=380, v_num=4, train_loss_step=274.0, val_loss=136.0, train_loss_e\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:  97%|▉| 30/31 [00:10<00:00,  2.88it/s, loss=355, v_num=4, train_loss_step=314.0, val_loss=136.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 31/31 [00:11<00:00,  2.73it/s, loss=355, v_num=4, train_loss_step=314.0, val_loss=117.0, train_loss_e\u001b[A\n",
      "Epoch 32:  97%|▉| 30/31 [00:10<00:00,  2.88it/s, loss=367, v_num=4, train_loss_step=313.0, val_loss=117.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 31/31 [00:12<00:00,  2.49it/s, loss=367, v_num=4, train_loss_step=313.0, val_loss=119.0, train_loss_e\u001b[A\n",
      "Epoch 33:  97%|▉| 30/31 [00:10<00:00,  2.95it/s, loss=367, v_num=4, train_loss_step=297.0, val_loss=119.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 31/31 [00:11<00:00,  2.80it/s, loss=367, v_num=4, train_loss_step=297.0, val_loss=111.0, train_loss_e\u001b[A\n",
      "Epoch 34:  97%|▉| 30/31 [00:11<00:00,  2.72it/s, loss=361, v_num=4, train_loss_step=410.0, val_loss=111.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 31/31 [00:12<00:00,  2.58it/s, loss=361, v_num=4, train_loss_step=410.0, val_loss=110.0, train_loss_e\u001b[A\n",
      "Epoch 35:  97%|▉| 30/31 [00:11<00:00,  2.61it/s, loss=366, v_num=4, train_loss_step=478.0, val_loss=110.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 31/31 [00:12<00:00,  2.48it/s, loss=366, v_num=4, train_loss_step=478.0, val_loss=135.0, train_loss_e\u001b[A\n",
      "Epoch 36:  97%|▉| 30/31 [00:15<00:00,  1.96it/s, loss=366, v_num=4, train_loss_step=365.0, val_loss=135.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 31/31 [00:16<00:00,  1.88it/s, loss=366, v_num=4, train_loss_step=365.0, val_loss=121.0, train_loss_e\u001b[A\n",
      "Epoch 37:  97%|▉| 30/31 [00:13<00:00,  2.19it/s, loss=336, v_num=4, train_loss_step=206.0, val_loss=121.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 31/31 [00:14<00:00,  2.10it/s, loss=336, v_num=4, train_loss_step=206.0, val_loss=71.30, train_loss_e\u001b[A\n",
      "Epoch 38:  97%|▉| 30/31 [00:13<00:00,  2.24it/s, loss=327, v_num=4, train_loss_step=416.0, val_loss=71.30, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 31/31 [00:14<00:00,  2.15it/s, loss=327, v_num=4, train_loss_step=416.0, val_loss=66.90, train_loss_e\u001b[A\n",
      "Epoch 39:  97%|▉| 30/31 [00:13<00:00,  2.25it/s, loss=321, v_num=4, train_loss_step=391.0, val_loss=66.90, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 31/31 [00:14<00:00,  2.13it/s, loss=321, v_num=4, train_loss_step=391.0, val_loss=67.30, train_loss_e\u001b[A\n",
      "Epoch 40:  97%|▉| 30/31 [00:10<00:00,  2.83it/s, loss=317, v_num=4, train_loss_step=420.0, val_loss=67.30, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 31/31 [00:11<00:00,  2.68it/s, loss=317, v_num=4, train_loss_step=420.0, val_loss=62.80, train_loss_e\u001b[A\n",
      "Epoch 41:  97%|▉| 30/31 [00:11<00:00,  2.66it/s, loss=315, v_num=4, train_loss_step=263.0, val_loss=62.80, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=315, v_num=4, train_loss_step=263.0, val_loss=68.40, train_loss_e\u001b[A\n",
      "Epoch 42:  97%|▉| 30/31 [00:10<00:00,  2.96it/s, loss=314, v_num=4, train_loss_step=147.0, val_loss=68.40, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 31/31 [00:11<00:00,  2.80it/s, loss=314, v_num=4, train_loss_step=147.0, val_loss=86.10, train_loss_e\u001b[A\n",
      "Epoch 43:  97%|▉| 30/31 [00:13<00:00,  2.26it/s, loss=327, v_num=4, train_loss_step=357.0, val_loss=86.10, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 31/31 [00:14<00:00,  2.17it/s, loss=327, v_num=4, train_loss_step=357.0, val_loss=107.0, train_loss_e\u001b[A\n",
      "Epoch 44:  97%|▉| 30/31 [00:10<00:00,  2.78it/s, loss=339, v_num=4, train_loss_step=433.0, val_loss=107.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 31/31 [00:11<00:00,  2.64it/s, loss=339, v_num=4, train_loss_step=433.0, val_loss=71.40, train_loss_e\u001b[A\n",
      "Epoch 45:  97%|▉| 30/31 [00:10<00:00,  2.73it/s, loss=301, v_num=4, train_loss_step=303.0, val_loss=71.40, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 31/31 [00:11<00:00,  2.59it/s, loss=301, v_num=4, train_loss_step=303.0, val_loss=57.70, train_loss_e\u001b[A\n",
      "Epoch 46:  97%|▉| 30/31 [00:11<00:00,  2.62it/s, loss=326, v_num=4, train_loss_step=327.0, val_loss=57.70, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 31/31 [00:12<00:00,  2.49it/s, loss=326, v_num=4, train_loss_step=327.0, val_loss=67.50, train_loss_e\u001b[A\n",
      "Epoch 47:  97%|▉| 30/31 [00:11<00:00,  2.59it/s, loss=319, v_num=4, train_loss_step=247.0, val_loss=67.50, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 31/31 [00:12<00:00,  2.45it/s, loss=319, v_num=4, train_loss_step=247.0, val_loss=75.40, train_loss_e\u001b[A\n",
      "Epoch 48:  97%|▉| 30/31 [00:12<00:00,  2.31it/s, loss=309, v_num=4, train_loss_step=306.0, val_loss=75.40, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 31/31 [00:13<00:00,  2.22it/s, loss=309, v_num=4, train_loss_step=306.0, val_loss=71.10, train_loss_e\u001b[A\n",
      "Epoch 49:  97%|▉| 30/31 [00:16<00:00,  1.82it/s, loss=333, v_num=4, train_loss_step=327.0, val_loss=71.10, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 31/31 [00:17<00:00,  1.78it/s, loss=333, v_num=4, train_loss_step=327.0, val_loss=71.30, train_loss_e\u001b[A\n",
      "Epoch 49: 100%|█| 31/31 [00:18<00:00,  1.72it/s, loss=333, v_num=4, train_loss_step=327.0, val_loss=71.30, train_loss_e\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|█| 31/31 [00:18<00:00,  1.69it/s, loss=333, v_num=4, train_loss_step=327.0, val_loss=71.30, train_loss_e\n"
     ]
    }
   ],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0cddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544ec259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19757.6543, 19776.3711, 19728.0664]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7fbdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trend_forecasting=[19757.6543, 19776.3711, 19728.0664]\n",
    "Trend_true=[19879.3770, 19879.0254, 19877.6895]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c4e09",
   "metadata": {},
   "source": [
    "## Seasonality\n",
    "#### The following code is a repetition of the 3rd to 10th code blocks from the same notebook, adjusted to predict the 'Seasonal' component instead of 'Trend'. The 'target' and 'time_varying_unknown_reals' parameters are modified accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4abc1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 4481],  # Filter data up to a certain time index for training\n",
    "    time_idx=\"time_idx\",  # The name of the column that provides the time index\n",
    "    target=\"Seasonal\",  # Set 'Seasonal' as the target variable for prediction.\n",
    "    min_encoder_length=max_encoder_length // 2,  # Minimum length of the encoder, half of the max_encoder_length\n",
    "    max_encoder_length=max_encoder_length,  # Maximum length of historical data used for predictions\n",
    "    min_prediction_length=1,  # Minimum prediction length\n",
    "    max_prediction_length=max_prediction_length,  # Maximum prediction length\n",
    "    time_varying_known_categoricals=[\"month\", \"day of the week\", \"day\", \"Holiday\"],  # Known categorical features\n",
    "    time_varying_known_reals=[\"time_idx\"],  # Known real features, like time index\n",
    "    time_varying_unknown_categoricals=[],  # Unknown categorical features, if any\n",
    "    time_varying_unknown_reals=[\"Seasonal\"],  # Set 'Seasonal' as the unknown real feature.\n",
    "    group_ids=['destination'],  # The column that identifies the time series group\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"  # A normalization technique\n",
    "    ),\n",
    "    add_relative_time_idx=True,  # Adds a column for relative time index\n",
    "    add_target_scales=True,  # Adds columns for target scales\n",
    "    add_encoder_length=True,  # Adds a column for encoder length\n",
    "    allow_missing_timesteps=True,  # Allows model to handle missing timesteps if there are any\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9f8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 858.9k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.03911626926390909,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.00842077448532244,  # Learning rate of the model\n",
    "    hidden_size=125,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.15160823136480017,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=17,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd25e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 539   \n",
      "3  | prescalers                         | ModuleDict                      | 204   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 17.5 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.5 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 14.3 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 63.2 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 63.2 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 63.2 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 63.2 K\n",
      "11 | lstm_encoder                       | LSTM                            | 126 K \n",
      "12 | lstm_decoder                       | LSTM                            | 126 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 31.5 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 250   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 78.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 62.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 31.8 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 63.2 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 31.8 K\n",
      "20 | output_layer                       | Linear                          | 882   \n",
      "----------------------------------------------------------------------------------------\n",
      "858 K     Trainable params\n",
      "0         Non-trainable params\n",
      "858 K     Total params\n",
      "3.436     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|██████████████████████████████ | 30/31 [00:10<00:00,  2.77it/s, loss=283, v_num=5, train_loss_step=236.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████████| 31/31 [00:11<00:00,  2.63it/s, loss=283, v_num=5, train_loss_step=236.0, val_loss=80.20]\u001b[A\n",
      "Epoch 1:  97%|▉| 30/31 [00:11<00:00,  2.72it/s, loss=199, v_num=5, train_loss_step=181.0, val_loss=80.20, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 31/31 [00:12<00:00,  2.58it/s, loss=199, v_num=5, train_loss_step=181.0, val_loss=73.70, train_loss_ep\u001b[A\n",
      "Epoch 2:  97%|▉| 30/31 [00:10<00:00,  2.78it/s, loss=197, v_num=5, train_loss_step=193.0, val_loss=73.70, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 31/31 [00:11<00:00,  2.63it/s, loss=197, v_num=5, train_loss_step=193.0, val_loss=70.70, train_loss_ep\u001b[A\n",
      "Epoch 3:  97%|▉| 30/31 [00:11<00:00,  2.67it/s, loss=197, v_num=5, train_loss_step=174.0, val_loss=70.70, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=197, v_num=5, train_loss_step=174.0, val_loss=67.60, train_loss_ep\u001b[A\n",
      "Epoch 4:  97%|▉| 30/31 [00:10<00:00,  2.84it/s, loss=190, v_num=5, train_loss_step=173.0, val_loss=67.60, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 31/31 [00:11<00:00,  2.67it/s, loss=190, v_num=5, train_loss_step=173.0, val_loss=68.60, train_loss_ep\u001b[A\n",
      "Epoch 5:  97%|▉| 30/31 [00:11<00:00,  2.69it/s, loss=188, v_num=5, train_loss_step=221.0, val_loss=68.60, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 31/31 [00:12<00:00,  2.57it/s, loss=188, v_num=5, train_loss_step=221.0, val_loss=68.50, train_loss_ep\u001b[A\n",
      "Epoch 6:  97%|▉| 30/31 [00:11<00:00,  2.59it/s, loss=194, v_num=5, train_loss_step=184.0, val_loss=68.50, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 31/31 [00:12<00:00,  2.47it/s, loss=194, v_num=5, train_loss_step=184.0, val_loss=77.20, train_loss_ep\u001b[A\n",
      "Epoch 7:  97%|▉| 30/31 [00:10<00:00,  2.80it/s, loss=182, v_num=5, train_loss_step=184.0, val_loss=77.20, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 31/31 [00:11<00:00,  2.66it/s, loss=182, v_num=5, train_loss_step=184.0, val_loss=74.40, train_loss_ep\u001b[A\n",
      "Epoch 8:  97%|▉| 30/31 [00:11<00:00,  2.60it/s, loss=183, v_num=5, train_loss_step=206.0, val_loss=74.40, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 31/31 [00:12<00:00,  2.46it/s, loss=183, v_num=5, train_loss_step=206.0, val_loss=79.50, train_loss_ep\u001b[A\n",
      "Epoch 9:  97%|▉| 30/31 [00:10<00:00,  2.75it/s, loss=178, v_num=5, train_loss_step=193.0, val_loss=79.50, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 31/31 [00:12<00:00,  2.51it/s, loss=178, v_num=5, train_loss_step=193.0, val_loss=72.70, train_loss_ep\u001b[A\n",
      "Epoch 10:  97%|▉| 30/31 [00:11<00:00,  2.68it/s, loss=178, v_num=5, train_loss_step=169.0, val_loss=72.70, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=178, v_num=5, train_loss_step=169.0, val_loss=77.30, train_loss_e\u001b[A\n",
      "Epoch 11:  97%|▉| 30/31 [00:11<00:00,  2.67it/s, loss=175, v_num=5, train_loss_step=158.0, val_loss=77.30, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=175, v_num=5, train_loss_step=158.0, val_loss=72.30, train_loss_e\u001b[A\n",
      "Epoch 12:  97%|▉| 30/31 [00:11<00:00,  2.67it/s, loss=175, v_num=5, train_loss_step=176.0, val_loss=72.30, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=175, v_num=5, train_loss_step=176.0, val_loss=76.80, train_loss_e\u001b[A\n",
      "Epoch 13:  97%|▉| 30/31 [00:11<00:00,  2.66it/s, loss=172, v_num=5, train_loss_step=148.0, val_loss=76.80, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 31/31 [00:12<00:00,  2.52it/s, loss=172, v_num=5, train_loss_step=148.0, val_loss=74.50, train_loss_e\u001b[A\n",
      "Epoch 13: 100%|█| 31/31 [00:13<00:00,  2.24it/s, loss=172, v_num=5, train_loss_step=148.0, val_loss=74.50, train_loss_e\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded1994a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter notebook\\\\saved_models\\\\best_model_epoch=3.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1785ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7085.0757, 6965.2119, 7035.4448]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab4240c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seasonal_forecasting=[7085.0757, 6965.2119, 7035.4448]\n",
    "Seasonal_true=[7073.4546, 6815.4194, 6926.9561]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4436",
   "metadata": {},
   "source": [
    "# Resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d666296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 4481],  # Filter data up to a certain time index for training\n",
    "    time_idx=\"time_idx\",  # The name of the column that provides the time index\n",
    "    target=\"Resid\",  # The target column to predict (one of the components from the RobustSTL decomposition)\n",
    "    min_encoder_length=max_encoder_length // 2,  # Minimum length of the encoder, half of the max_encoder_length\n",
    "    max_encoder_length=max_encoder_length,  # Maximum length of historical data used for predictions\n",
    "    min_prediction_length=1,  # Minimum prediction length\n",
    "    max_prediction_length=max_prediction_length,  # Maximum prediction length\n",
    "    time_varying_known_categoricals=[\"month\", \"day of the week\", \"day\", \"Holiday\"],  # Known categorical features\n",
    "    time_varying_known_reals=[\"time_idx\"],  # Known real features, like time index\n",
    "    time_varying_unknown_categoricals=[],  # Unknown categorical features, if any\n",
    "    time_varying_unknown_reals=[\"Resid\"],  # Unknown real features, which includes the target 'Trend' itself\n",
    "    group_ids=['destination'],  # The column that identifies the time series group\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"  # A normalization technique\n",
    "    ),\n",
    "    add_relative_time_idx=True,  # Adds a column for relative time index\n",
    "    add_target_scales=True,  # Adds columns for target scales\n",
    "    add_encoder_length=True,  # Adds a column for encoder length\n",
    "    allow_missing_timesteps=True,  # Allows model to handle missing timesteps if there are any\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d65ebf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0785ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 858.9k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.03911626926390909,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.00842077448532244,  # Learning rate of the model\n",
    "    hidden_size=125,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.15160823136480017,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=17,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee06564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 539   \n",
      "3  | prescalers                         | ModuleDict                      | 204   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 17.5 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.5 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 14.3 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 63.2 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 63.2 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 63.2 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 63.2 K\n",
      "11 | lstm_encoder                       | LSTM                            | 126 K \n",
      "12 | lstm_decoder                       | LSTM                            | 126 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 31.5 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 250   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 78.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 62.9 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 31.8 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 63.2 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 31.8 K\n",
      "20 | output_layer                       | Linear                          | 882   \n",
      "----------------------------------------------------------------------------------------\n",
      "858 K     Trainable params\n",
      "0         Non-trainable params\n",
      "858 K     Total params\n",
      "3.436     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|██████████████████████████████ | 30/31 [00:11<00:00,  2.69it/s, loss=367, v_num=6, train_loss_step=309.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████████| 31/31 [00:12<00:00,  2.56it/s, loss=367, v_num=6, train_loss_step=309.0, val_loss=296.0]\u001b[A\n",
      "Epoch 1:  97%|▉| 30/31 [00:11<00:00,  2.72it/s, loss=268, v_num=6, train_loss_step=241.0, val_loss=296.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 31/31 [00:12<00:00,  2.57it/s, loss=268, v_num=6, train_loss_step=241.0, val_loss=268.0, train_loss_ep\u001b[A\n",
      "Epoch 2:  97%|▉| 30/31 [00:10<00:00,  2.77it/s, loss=213, v_num=6, train_loss_step=183.0, val_loss=268.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 31/31 [00:11<00:00,  2.61it/s, loss=213, v_num=6, train_loss_step=183.0, val_loss=202.0, train_loss_ep\u001b[A\n",
      "Epoch 3:  97%|▉| 30/31 [00:11<00:00,  2.68it/s, loss=151, v_num=6, train_loss_step=131.0, val_loss=202.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 31/31 [00:12<00:00,  2.55it/s, loss=151, v_num=6, train_loss_step=131.0, val_loss=160.0, train_loss_ep\u001b[A\n",
      "Epoch 4:  97%|▉| 30/31 [00:12<00:00,  2.41it/s, loss=119, v_num=6, train_loss_step=117.0, val_loss=160.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 31/31 [00:13<00:00,  2.31it/s, loss=119, v_num=6, train_loss_step=117.0, val_loss=176.0, train_loss_ep\u001b[A\n",
      "Epoch 5:  97%|▉| 30/31 [00:10<00:00,  2.80it/s, loss=104, v_num=6, train_loss_step=95.90, val_loss=176.0, train_loss_ep\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 31/31 [00:11<00:00,  2.65it/s, loss=104, v_num=6, train_loss_step=95.90, val_loss=173.0, train_loss_ep\u001b[A\n",
      "Epoch 6:  97%|▉| 30/31 [00:10<00:00,  2.80it/s, loss=93.9, v_num=6, train_loss_step=95.50, val_loss=173.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 31/31 [00:11<00:00,  2.64it/s, loss=93.9, v_num=6, train_loss_step=95.50, val_loss=142.0, train_loss_e\u001b[A\n",
      "Epoch 7:  97%|▉| 30/31 [00:11<00:00,  2.59it/s, loss=86.6, v_num=6, train_loss_step=84.60, val_loss=142.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 31/31 [00:12<00:00,  2.46it/s, loss=86.6, v_num=6, train_loss_step=84.60, val_loss=180.0, train_loss_e\u001b[A\n",
      "Epoch 8:  97%|▉| 30/31 [00:11<00:00,  2.72it/s, loss=83, v_num=6, train_loss_step=86.30, val_loss=180.0, train_loss_epo\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 31/31 [00:12<00:00,  2.57it/s, loss=83, v_num=6, train_loss_step=86.30, val_loss=151.0, train_loss_epo\u001b[A\n",
      "Epoch 9:  97%|▉| 30/31 [00:10<00:00,  2.73it/s, loss=83.3, v_num=6, train_loss_step=82.20, val_loss=151.0, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 31/31 [00:12<00:00,  2.58it/s, loss=83.3, v_num=6, train_loss_step=82.20, val_loss=128.0, train_loss_e\u001b[A\n",
      "Epoch 10:  97%|▉| 30/31 [00:10<00:00,  2.73it/s, loss=80.8, v_num=6, train_loss_step=77.30, val_loss=128.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 31/31 [00:11<00:00,  2.60it/s, loss=80.8, v_num=6, train_loss_step=77.30, val_loss=151.0, train_loss_\u001b[A\n",
      "Epoch 11:  97%|▉| 30/31 [00:10<00:00,  2.76it/s, loss=79.8, v_num=6, train_loss_step=81.90, val_loss=151.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 31/31 [00:11<00:00,  2.63it/s, loss=79.8, v_num=6, train_loss_step=81.90, val_loss=200.0, train_loss_\u001b[A\n",
      "Epoch 12:  97%|▉| 30/31 [00:10<00:00,  2.83it/s, loss=74.6, v_num=6, train_loss_step=75.60, val_loss=200.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 31/31 [00:11<00:00,  2.67it/s, loss=74.6, v_num=6, train_loss_step=75.60, val_loss=158.0, train_loss_\u001b[A\n",
      "Epoch 13:  97%|▉| 30/31 [00:10<00:00,  2.77it/s, loss=72.1, v_num=6, train_loss_step=73.80, val_loss=158.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 31/31 [00:11<00:00,  2.63it/s, loss=72.1, v_num=6, train_loss_step=73.80, val_loss=154.0, train_loss_\u001b[A\n",
      "Epoch 14:  97%|▉| 30/31 [00:11<00:00,  2.71it/s, loss=68.2, v_num=6, train_loss_step=70.20, val_loss=154.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 31/31 [00:12<00:00,  2.57it/s, loss=68.2, v_num=6, train_loss_step=70.20, val_loss=172.0, train_loss_\u001b[A\n",
      "Epoch 15:  97%|▉| 30/31 [00:11<00:00,  2.59it/s, loss=61.3, v_num=6, train_loss_step=61.20, val_loss=172.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 31/31 [00:12<00:00,  2.46it/s, loss=61.3, v_num=6, train_loss_step=61.20, val_loss=160.0, train_loss_\u001b[A\n",
      "Epoch 16:  97%|▉| 30/31 [00:11<00:00,  2.65it/s, loss=60.5, v_num=6, train_loss_step=60.90, val_loss=160.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 31/31 [00:12<00:00,  2.51it/s, loss=60.5, v_num=6, train_loss_step=60.90, val_loss=166.0, train_loss_\u001b[A\n",
      "Epoch 17:  97%|▉| 30/31 [00:11<00:00,  2.67it/s, loss=56.9, v_num=6, train_loss_step=67.50, val_loss=166.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 31/31 [00:12<00:00,  2.53it/s, loss=56.9, v_num=6, train_loss_step=67.50, val_loss=158.0, train_loss_\u001b[A\n",
      "Epoch 18:  97%|▉| 30/31 [00:11<00:00,  2.62it/s, loss=54.1, v_num=6, train_loss_step=49.40, val_loss=158.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 31/31 [00:12<00:00,  2.48it/s, loss=54.1, v_num=6, train_loss_step=49.40, val_loss=140.0, train_loss_\u001b[A\n",
      "Epoch 19:  97%|▉| 30/31 [00:11<00:00,  2.63it/s, loss=52.3, v_num=6, train_loss_step=54.30, val_loss=140.0, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 31/31 [00:12<00:00,  2.49it/s, loss=52.3, v_num=6, train_loss_step=54.30, val_loss=155.0, train_loss_\u001b[A\n",
      "Epoch 19: 100%|█| 31/31 [00:13<00:00,  2.30it/s, loss=52.3, v_num=6, train_loss_step=54.30, val_loss=155.0, train_loss_\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8db5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5c54465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3611.5913, 2123.0720,  312.2786]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "755503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resid_forecasting=[3611.5913, 2123.0720,  312.2786]\n",
    "Resid_true=[3735.1682, 2277.5544,  299.3539]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1321b9c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e1fe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tourist: [30454.3213, 28864.655, 27075.789800000002]\n",
      "tourist_t: [30687.9998, 28971.9992, 27103.999499999998]\n",
      "MAE: 123.07746666666571\n",
      "RMSE: 149.35885846954923\n",
      "MAPE: 0.41201834338207105\n"
     ]
    }
   ],
   "source": [
    "# Combine the forecasted 'Trend', 'Seasonal', and 'Resid' components to obtain the forecasted 'tourist' numbers.\n",
    "# The comprehension list sums the corresponding values from the three forecasts for each point in time.\n",
    "\n",
    "import numpy as np\n",
    "tourist = [sum(x) for x in zip(Trend_forecasting, Seasonal_forecasting, Resid_forecasting)]\n",
    "\n",
    "# Similarly, combine the true 'Trend', 'Seasonal', and 'Resid' components to obtain the true 'tourist' numbers.\n",
    "# This is typically used for validation or testing purposes to compare against the forecasted values.\n",
    "tourist_t = [sum(x) for x in zip(Trend_true, Seasonal_true, Resid_true)]\n",
    "\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) to evaluate the accuracy of the forecasts.\n",
    "# These metrics provide a quantitative measure of the model's predictive performance.\n",
    "mae = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)))\n",
    "rmse = np.sqrt(np.mean(np.square(np.array(tourist_t) - np.array(tourist))))\n",
    "mape = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)) / np.array(tourist_t)) * 100\n",
    "\n",
    "# Output the forecasted and true 'tourist' numbers, as well as the calculated error metrics.\n",
    "print(\"tourist:\", tourist)\n",
    "print(\"tourist_t:\", tourist_t)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd97e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
