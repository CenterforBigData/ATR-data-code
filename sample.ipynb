{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3eeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6d8b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tourist</th>\n",
       "      <th>date</th>\n",
       "      <th>lag</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>pc</th>\n",
       "      <th>mob</th>\n",
       "      <th>weekday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>holiday</th>\n",
       "      <th>destination</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Seasonal</th>\n",
       "      <th>Resid</th>\n",
       "      <th>seasonal</th>\n",
       "      <th>resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279.0</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>858</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>290.691365</td>\n",
       "      <td>3970.854466</td>\n",
       "      <td>5017.454170</td>\n",
       "      <td>-29.145534</td>\n",
       "      <td>17.454170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208.0</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>388</td>\n",
       "      <td>856</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>318.851010</td>\n",
       "      <td>3934.201452</td>\n",
       "      <td>4954.947538</td>\n",
       "      <td>-65.798548</td>\n",
       "      <td>-45.052462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295.0</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>445</td>\n",
       "      <td>873</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>347.046695</td>\n",
       "      <td>3948.992652</td>\n",
       "      <td>4998.960653</td>\n",
       "      <td>-51.007348</td>\n",
       "      <td>-1.039347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311.0</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>333</td>\n",
       "      <td>877</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>375.607632</td>\n",
       "      <td>3868.047896</td>\n",
       "      <td>5067.344472</td>\n",
       "      <td>-131.952104</td>\n",
       "      <td>67.344472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528.0</td>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>218</td>\n",
       "      <td>945</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Tomb_Sweeping_Day</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>405.057652</td>\n",
       "      <td>4138.158943</td>\n",
       "      <td>4984.783405</td>\n",
       "      <td>138.158943</td>\n",
       "      <td>-15.216595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>18271.0</td>\n",
       "      <td>2022-08-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>869</td>\n",
       "      <td>586</td>\n",
       "      <td>2795</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>17650.768622</td>\n",
       "      <td>4471.042606</td>\n",
       "      <td>5149.188772</td>\n",
       "      <td>471.042606</td>\n",
       "      <td>149.188772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>19132.0</td>\n",
       "      <td>2022-08-17</td>\n",
       "      <td>-1</td>\n",
       "      <td>870</td>\n",
       "      <td>685</td>\n",
       "      <td>2871</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>17687.353387</td>\n",
       "      <td>7904.887545</td>\n",
       "      <td>2539.759068</td>\n",
       "      <td>3904.887545</td>\n",
       "      <td>-2460.240932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>15209.0</td>\n",
       "      <td>2022-08-18</td>\n",
       "      <td>-1</td>\n",
       "      <td>871</td>\n",
       "      <td>2868</td>\n",
       "      <td>3588</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>17721.182972</td>\n",
       "      <td>2244.456347</td>\n",
       "      <td>4243.360682</td>\n",
       "      <td>-1755.543653</td>\n",
       "      <td>-756.639318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>15902.0</td>\n",
       "      <td>2022-08-19</td>\n",
       "      <td>-1</td>\n",
       "      <td>872</td>\n",
       "      <td>1102</td>\n",
       "      <td>3233</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>17752.081802</td>\n",
       "      <td>2100.969245</td>\n",
       "      <td>5048.948953</td>\n",
       "      <td>-1899.030755</td>\n",
       "      <td>48.948953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>14086.0</td>\n",
       "      <td>2022-08-20</td>\n",
       "      <td>-1</td>\n",
       "      <td>873</td>\n",
       "      <td>344</td>\n",
       "      <td>2801</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>Normal</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>17781.772518</td>\n",
       "      <td>230.676397</td>\n",
       "      <td>5073.551085</td>\n",
       "      <td>-3769.323603</td>\n",
       "      <td>73.551085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tourist        date  lag  time_idx    pc   mob    weekday  year month  \\\n",
       "0      279.0  2020-03-31   -1         1   367   858     Monday  2020     3   \n",
       "1      208.0  2020-04-01   -1         2   388   856    Tuesday  2020     4   \n",
       "2      295.0  2020-04-02   -1         3   445   873  Wednesday  2020     4   \n",
       "3      311.0  2020-04-03   -1         4   333   877   Thursday  2020     4   \n",
       "4      528.0  2020-04-04   -1         5   218   945     Friday  2020     4   \n",
       "..       ...         ...  ...       ...   ...   ...        ...   ...   ...   \n",
       "868  18271.0  2022-08-16   -1       869   586  2795     Monday  2022     8   \n",
       "869  19132.0  2022-08-17   -1       870   685  2871    Tuesday  2022     8   \n",
       "870  15209.0  2022-08-18   -1       871  2868  3588  Wednesday  2022     8   \n",
       "871  15902.0  2022-08-19   -1       872  1102  3233   Thursday  2022     8   \n",
       "872  14086.0  2022-08-20   -1       873   344  2801     Friday  2022     8   \n",
       "\n",
       "    day            holiday destination         Trend     Seasonal  \\\n",
       "0    31             Normal  JiuZhaigou    290.691365  3970.854466   \n",
       "1     1             Normal  JiuZhaigou    318.851010  3934.201452   \n",
       "2     2             Normal  JiuZhaigou    347.046695  3948.992652   \n",
       "3     3             Normal  JiuZhaigou    375.607632  3868.047896   \n",
       "4     4  Tomb_Sweeping_Day  JiuZhaigou    405.057652  4138.158943   \n",
       "..   ..                ...         ...           ...          ...   \n",
       "868  16             Normal  JiuZhaigou  17650.768622  4471.042606   \n",
       "869  17             Normal  JiuZhaigou  17687.353387  7904.887545   \n",
       "870  18             Normal  JiuZhaigou  17721.182972  2244.456347   \n",
       "871  19             Normal  JiuZhaigou  17752.081802  2100.969245   \n",
       "872  20             Normal  JiuZhaigou  17781.772518   230.676397   \n",
       "\n",
       "           Resid     seasonal        resid  \n",
       "0    5017.454170   -29.145534    17.454170  \n",
       "1    4954.947538   -65.798548   -45.052462  \n",
       "2    4998.960653   -51.007348    -1.039347  \n",
       "3    5067.344472  -131.952104    67.344472  \n",
       "4    4984.783405   138.158943   -15.216595  \n",
       "..           ...          ...          ...  \n",
       "868  5149.188772   471.042606   149.188772  \n",
       "869  2539.759068  3904.887545 -2460.240932  \n",
       "870  4243.360682 -1755.543653  -756.639318  \n",
       "871  5048.948953 -1899.030755    48.948953  \n",
       "872  5073.551085 -3769.323603    73.551085  \n",
       "\n",
       "[873 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the excel file\n",
    "data = pd.read_excel('C:/Users/xuyechi/Desktop/科研/tft/九寨沟客流量.xlsx')\n",
    "# df[\"Date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "# df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) // 10**9\n",
    "data[\"year\"] = data[\"year\"].astype(str)\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data['holiday'] = data['holiday'].astype(str)\n",
    "data[\"tourist\"] = data[\"tourist\"].astype(\"float64\")\n",
    "data[\"Trend\"]=data[\"Trend\"].astype(\"float64\")\n",
    "data[\"Seasonal\"]=data[\"Seasonal\"].astype(\"float64\")\n",
    "data[\"Resid\"]=data[\"Resid\"].astype(\"float64\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc00e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 1\n",
    "max_encoder_length = 30\n",
    "# training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 783],\n",
    "    #data.iloc[3925:training_cutoff,:],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Seasonal\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\",\"holiday\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc\",\"mob\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Seasonal\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "     # 取了对数\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")\n",
    "\n",
    "# # inverse transform\n",
    "# forecast = predictor.predict(test_data)\n",
    "# forecast_samples = forecast.samples\n",
    "# forecast_samples = training.target_normalizer.inverse_transform(forecast_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a984db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "# from_dataset是生成具有不同底层数据但相同变量编码器和标量的数据集的函数\n",
    "# training 代表validation所需要的参数； data代表validation从哪选；\n",
    "\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "\n",
    "\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# move dataloaders to device\n",
    "\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10 , num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979cd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-02 20:43:24,190]\u001b[0m A new study created in memory with name: no-name-ff787481-b797-4e76-86e7-90f7cc348fda\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-04-02 20:44:05,741]\u001b[0m Trial 0 finished with value: 2302.072265625 and parameters: {'gradient_clip_val': 0.40786515045691396, 'hidden_size': 118, 'dropout': 0.11570947335393039, 'hidden_continuous_size': 23, 'attention_head_size': 4, 'learning_rate': 0.07761772416675755}. Best is trial 0 with value: 2302.072265625.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-04-02 20:44:48,476]\u001b[0m Trial 1 finished with value: 1747.357421875 and parameters: {'gradient_clip_val': 0.31330228284676426, 'hidden_size': 15, 'dropout': 0.2757684537904421, 'hidden_continuous_size': 10, 'attention_head_size': 2, 'learning_rate': 0.0011749674283127688}. Best is trial 1 with value: 1747.357421875.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-04-02 20:45:27,185]\u001b[0m Trial 2 finished with value: 1761.191650390625 and parameters: {'gradient_clip_val': 0.336491009350642, 'hidden_size': 9, 'dropout': 0.1458957521386966, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.0010301860036872647}. Best is trial 1 with value: 1747.357421875.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:29,051]\u001b[0m Trial 3 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:30,795]\u001b[0m Trial 4 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:45,000]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:46,820]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:48,561]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:45:50,319]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:04,262]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:06,086]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:07,865]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:09,672]\u001b[0m Trial 12 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:11,256]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:13,162]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:14,892]\u001b[0m Trial 15 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:16,640]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:18,467]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:20,205]\u001b[0m Trial 18 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:21,870]\u001b[0m Trial 19 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:23,868]\u001b[0m Trial 20 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:39,681]\u001b[0m Trial 21 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:44,571]\u001b[0m Trial 22 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:46,440]\u001b[0m Trial 23 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:51,003]\u001b[0m Trial 24 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:52,985]\u001b[0m Trial 25 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:54,791]\u001b[0m Trial 26 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:56,591]\u001b[0m Trial 27 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:46:58,350]\u001b[0m Trial 28 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:00,221]\u001b[0m Trial 29 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:01,999]\u001b[0m Trial 30 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:03,760]\u001b[0m Trial 31 pruned. Trial was pruned at epoch 1.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:05,520]\u001b[0m Trial 32 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:10,065]\u001b[0m Trial 33 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:15,205]\u001b[0m Trial 34 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:17,495]\u001b[0m Trial 35 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:19,485]\u001b[0m Trial 36 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:21,425]\u001b[0m Trial 37 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:23,402]\u001b[0m Trial 38 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:47:25,162]\u001b[0m Trial 39 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-04-02 20:48:12,789]\u001b[0m Trial 40 finished with value: 2050.01123046875 and parameters: {'gradient_clip_val': 0.3093780371241915, 'hidden_size': 32, 'dropout': 0.1473046987151529, 'hidden_continuous_size': 14, 'attention_head_size': 2, 'learning_rate': 0.07863411540189284}. Best is trial 1 with value: 1747.357421875.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:18,023]\u001b[0m Trial 41 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:20,183]\u001b[0m Trial 42 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:25,593]\u001b[0m Trial 43 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:27,793]\u001b[0m Trial 44 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:29,697]\u001b[0m Trial 45 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:31,492]\u001b[0m Trial 46 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:33,396]\u001b[0m Trial 47 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:35,457]\u001b[0m Trial 48 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-04-02 20:48:40,677]\u001b[0m Trial 49 pruned. Trial was pruned at epoch 4.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_clip_val': 0.31330228284676426, 'hidden_size': 15, 'dropout': 0.2757684537904421, 'hidden_continuous_size': 10, 'attention_head_size': 2, 'learning_rate': 0.0011749674283127688}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=50,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False\n",
    ")\n",
    "\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac411db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 \n",
    "#{'gradient_clip_val': 0.058711482089361826, 'hidden_size': 14, \n",
    "#'dropout': 0.2722111720167587, 'hidden_continuous_size': 10, 'attention_head_size': 2, 'learning_rate': 0.09285323457031537}\n",
    "\n",
    "#3 \n",
    "# {'gradient_clip_val': 0.011011271624431383, 'hidden_size': 30, \n",
    "#  'dropout': 0.2954455811444877, 'hidden_continuous_size': 14, 'attention_head_size': 2, 'learning_rate': 0.08100063625658273}\n",
    "\n",
    "\n",
    "# 7\n",
    "# {'gradient_clip_val': 0.06965995703621122, 'hidden_size': 70, \n",
    "#  'dropout': 0.20063425125576373, 'hidden_continuous_size': 18, 'attention_head_size': 1, 'learning_rate': 0.0969849703729482}\n",
    "\n",
    "#15\n",
    "# {'gradient_clip_val': 0.05170506339745095, 'hidden_size': 43,\n",
    "#  'dropout': 0.27510184522694925, 'hidden_continuous_size': 23, 'attention_head_size': 1, 'learning_rate': 0.003971445810850568}\n",
    "\n",
    "#30\n",
    "# {'gradient_clip_val': 0.11278609968246897, 'hidden_size': 61, \n",
    "#  'dropout': 0.17501843926436106, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.002323645204383734}\n",
    "\n",
    "# 60\n",
    "# {'gradient_clip_val': 0.08963621290452617, 'hidden_size': 90, \n",
    "#  'dropout': 0.24069089701689708, 'hidden_continuous_size': 22, 'attention_head_size': 2, 'learning_rate': 0.027940923926813188}\n",
    "\n",
    "# 90\n",
    "# {'gradient_clip_val': 0.01229433160487252, 'hidden_size': 99,\n",
    "#  'dropout': 0.21930970181368742, 'hidden_continuous_size': 90, 'attention_head_size': 4, 'learning_rate': 0.0033330257773551033}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dc2c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 23.0k\n"
     ]
    }
   ],
   "source": [
    "# 1天\n",
    "# {'gradient_clip_val': 0.31330228284676426, 'hidden_size': 15, \n",
    "# 'dropout': 0.2757684537904421, 'hidden_continuous_size': 10, 'attention_head_size': 2, 'learning_rate': 0.0011749674283127688}\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.31330228284676426,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.0011749674283127688,\n",
    "    hidden_size=15,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.2757684537904421,\n",
    "    hidden_continuous_size=10,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ba38e0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 69.1k\n"
     ]
    }
   ],
   "source": [
    "# 3 天\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "#3 \n",
    "# {'gradient_clip_val': 0.011011271624431383, 'hidden_size': 30, \n",
    "#  'dropout': 0.2954455811444877, 'hidden_continuous_size': 14, 'attention_head_size': 2, 'learning_rate': 0.08100063625658273}\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.011011271624431383,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.08100063625658273,\n",
    "    hidden_size=30,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.2954455811444877,\n",
    "    hidden_continuous_size=14,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc32894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 305.7k\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "# {'gradient_clip_val': 0.06965995703621122, 'hidden_size': 70, \n",
    "#  'dropout': 0.20063425125576373, 'hidden_continuous_size': 18, 'attention_head_size': 1, 'learning_rate': 0.0969849703729482}\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.06965995703621122,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.0269849703729482,\n",
    "    hidden_size=70,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.20063425125576373,\n",
    "    hidden_continuous_size=18,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=8,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33bfff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 143.8k\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "# {'gradient_clip_val': 0.05170506339745095, 'hidden_size': 43,\n",
    "#  'dropout': 0.27510184522694925, 'hidden_continuous_size': 23, 'attention_head_size': 1, 'learning_rate': 0.003971445810850568}\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.05170506339745095,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# 0.009971445810850568\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.003971445810850568,\n",
    "    hidden_size=43,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.27510184522694925,\n",
    "    hidden_continuous_size=23,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=8,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "# {'gradient_clip_val': 0.11278609968246897, 'hidden_size': 61, \n",
    "#  'dropout': 0.17501843926436106, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.002323645204383734}\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.11278609968246897,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# 0.009971445810850568\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.002323645204383734,\n",
    "    hidden_size=61,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.17501843926436106,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=8,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "# {'gradient_clip_val': 0.08963621290452617, 'hidden_size': 90, \n",
    "#  'dropout': 0.24069089701689708, 'hidden_continuous_size': 22, 'attention_head_size': 2, 'learning_rate': 0.027940923926813188}\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.08963621290452617,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# 0.009971445810850568\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.027940923926813188,\n",
    "    hidden_size=90,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.24069089701689708,\n",
    "    hidden_continuous_size=22,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=8,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90\n",
    "# {'gradient_clip_val': 0.01229433160487252, 'hidden_size': 99,\n",
    "#  'dropout': 0.21930970181368742, 'hidden_continuous_size': 90, 'attention_head_size': 4, 'learning_rate': 0.0033330257773551033}\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    save_top_k=1,  # 仅保留验证集上误差最小的模型\n",
    "    filename=\"best_model_{epoch}\",  # 您可以根据需要自定义文件名格式\n",
    "    dirpath=\"saved_models\"\n",
    ")\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.01229433160487252,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback,checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# 0.009971445810850568\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.0033330257773551033,\n",
    "    hidden_size=99,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.21930970181368742,\n",
    "    hidden_continuous_size=90,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=8,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 488   \n",
      "3  | prescalers                         | ModuleDict                      | 160   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 2.1 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 4.6 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 990   \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 990   \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 990   \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 990   \n",
      "11 | lstm_encoder                       | LSTM                            | 1.9 K \n",
      "12 | lstm_decoder                       | LSTM                            | 1.9 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 480   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 30    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.2 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 665   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 510   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 990   \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 510   \n",
      "20 | output_layer                       | Linear                          | 112   \n",
      "----------------------------------------------------------------------------------------\n",
      "23.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.0 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|██████████████████████████▌    | 6/7 [00:01<00:00,  4.96it/s, loss=165, v_num=140, train_loss_step=157.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████████| 7/7 [00:01<00:00,  4.43it/s, loss=165, v_num=140, train_loss_step=157.0, val_loss=1.9e+3]\u001b[A\n",
      "Epoch 1:  86%|▊| 6/7 [00:00<00:00,  6.72it/s, loss=166, v_num=140, train_loss_step=177.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 7/7 [00:01<00:00,  5.38it/s, loss=166, v_num=140, train_loss_step=177.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Epoch 2:  86%|▊| 6/7 [00:00<00:00,  6.15it/s, loss=165, v_num=140, train_loss_step=161.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 7/7 [00:01<00:00,  4.62it/s, loss=165, v_num=140, train_loss_step=161.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Epoch 3:  86%|▊| 6/7 [00:00<00:00,  6.71it/s, loss=163, v_num=140, train_loss_step=171.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 7/7 [00:01<00:00,  5.51it/s, loss=163, v_num=140, train_loss_step=171.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Epoch 4:  86%|▊| 6/7 [00:00<00:00,  6.82it/s, loss=163, v_num=140, train_loss_step=172.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 7/7 [00:01<00:00,  5.55it/s, loss=163, v_num=140, train_loss_step=172.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Epoch 5:  86%|▊| 6/7 [00:01<00:00,  5.14it/s, loss=160, v_num=140, train_loss_step=145.0, val_loss=1.9e+3, train_loss_e\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 7/7 [00:01<00:00,  4.56it/s, loss=160, v_num=140, train_loss_step=145.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Epoch 6:  86%|▊| 6/7 [00:00<00:00,  6.67it/s, loss=157, v_num=140, train_loss_step=154.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 7/7 [00:01<00:00,  5.41it/s, loss=157, v_num=140, train_loss_step=154.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Epoch 7:  86%|▊| 6/7 [00:00<00:00,  6.58it/s, loss=155, v_num=140, train_loss_step=158.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 7/7 [00:01<00:00,  5.43it/s, loss=155, v_num=140, train_loss_step=158.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Epoch 8:  86%|▊| 6/7 [00:00<00:00,  6.67it/s, loss=152, v_num=140, train_loss_step=146.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 7/7 [00:01<00:00,  5.47it/s, loss=152, v_num=140, train_loss_step=146.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Epoch 9:  86%|▊| 6/7 [00:01<00:00,  5.97it/s, loss=148, v_num=140, train_loss_step=152.0, val_loss=1.89e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 7/7 [00:01<00:00,  4.92it/s, loss=148, v_num=140, train_loss_step=152.0, val_loss=1.88e+3, train_loss_\u001b[A\n",
      "Epoch 10:  86%|▊| 6/7 [00:00<00:00,  6.50it/s, loss=147, v_num=140, train_loss_step=130.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 7/7 [00:01<00:00,  5.25it/s, loss=147, v_num=140, train_loss_step=130.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Epoch 11:  86%|▊| 6/7 [00:00<00:00,  6.63it/s, loss=143, v_num=140, train_loss_step=138.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 7/7 [00:01<00:00,  5.48it/s, loss=143, v_num=140, train_loss_step=138.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Epoch 12:  86%|▊| 6/7 [00:00<00:00,  6.86it/s, loss=141, v_num=140, train_loss_step=145.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 7/7 [00:01<00:00,  5.54it/s, loss=141, v_num=140, train_loss_step=145.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Epoch 13:  86%|▊| 6/7 [00:01<00:00,  5.02it/s, loss=137, v_num=140, train_loss_step=133.0, val_loss=1.88e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 7/7 [00:01<00:00,  4.43it/s, loss=137, v_num=140, train_loss_step=133.0, val_loss=1.87e+3, train_loss\u001b[A\n",
      "Epoch 14:  86%|▊| 6/7 [00:00<00:00,  6.68it/s, loss=135, v_num=140, train_loss_step=121.0, val_loss=1.87e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 7/7 [00:01<00:00,  5.48it/s, loss=135, v_num=140, train_loss_step=121.0, val_loss=1.87e+3, train_loss\u001b[A\n",
      "Epoch 15:  86%|▊| 6/7 [00:01<00:00,  5.99it/s, loss=131, v_num=140, train_loss_step=131.0, val_loss=1.87e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 7/7 [00:01<00:00,  4.92it/s, loss=131, v_num=140, train_loss_step=131.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Epoch 16:  86%|▊| 6/7 [00:00<00:00,  6.32it/s, loss=127, v_num=140, train_loss_step=144.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 7/7 [00:01<00:00,  5.22it/s, loss=127, v_num=140, train_loss_step=144.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Epoch 17:  86%|▊| 6/7 [00:00<00:00,  6.51it/s, loss=122, v_num=140, train_loss_step=115.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 7/7 [00:01<00:00,  5.29it/s, loss=122, v_num=140, train_loss_step=115.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Epoch 18:  86%|▊| 6/7 [00:00<00:00,  6.68it/s, loss=119, v_num=140, train_loss_step=116.0, val_loss=1.86e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 7/7 [00:01<00:00,  4.25it/s, loss=119, v_num=140, train_loss_step=116.0, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Epoch 19:  86%|▊| 6/7 [00:00<00:00,  6.31it/s, loss=117, v_num=140, train_loss_step=127.0, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 7/7 [00:01<00:00,  5.09it/s, loss=117, v_num=140, train_loss_step=127.0, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Epoch 20:  86%|▊| 6/7 [00:00<00:00,  6.51it/s, loss=113, v_num=140, train_loss_step=102.0, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 7/7 [00:01<00:00,  5.36it/s, loss=113, v_num=140, train_loss_step=102.0, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Epoch 21:  86%|▊| 6/7 [00:00<00:00,  6.09it/s, loss=111, v_num=140, train_loss_step=98.30, val_loss=1.85e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 7/7 [00:01<00:00,  5.11it/s, loss=111, v_num=140, train_loss_step=98.30, val_loss=1.84e+3, train_loss\u001b[A\n",
      "Epoch 22:  86%|▊| 6/7 [00:00<00:00,  6.82it/s, loss=109, v_num=140, train_loss_step=109.0, val_loss=1.84e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 7/7 [00:01<00:00,  5.53it/s, loss=109, v_num=140, train_loss_step=109.0, val_loss=1.84e+3, train_loss\u001b[A\n",
      "Epoch 23:  86%|▊| 6/7 [00:00<00:00,  6.08it/s, loss=106, v_num=140, train_loss_step=75.00, val_loss=1.84e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 7/7 [00:01<00:00,  4.86it/s, loss=106, v_num=140, train_loss_step=75.00, val_loss=1.83e+3, train_loss\u001b[A\n",
      "Epoch 24:  86%|▊| 6/7 [00:00<00:00,  6.18it/s, loss=105, v_num=140, train_loss_step=91.80, val_loss=1.83e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 7/7 [00:01<00:00,  4.98it/s, loss=105, v_num=140, train_loss_step=91.80, val_loss=1.83e+3, train_loss\u001b[A\n",
      "Epoch 25:  86%|▊| 6/7 [00:00<00:00,  6.29it/s, loss=101, v_num=140, train_loss_step=90.70, val_loss=1.83e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 7/7 [00:01<00:00,  3.89it/s, loss=101, v_num=140, train_loss_step=90.70, val_loss=1.83e+3, train_loss\u001b[A\n",
      "Epoch 26:  86%|▊| 6/7 [00:00<00:00,  6.25it/s, loss=97.5, v_num=140, train_loss_step=104.0, val_loss=1.83e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 7/7 [00:01<00:00,  5.13it/s, loss=97.5, v_num=140, train_loss_step=104.0, val_loss=1.82e+3, train_los\u001b[A\n",
      "Epoch 27:  86%|▊| 6/7 [00:01<00:00,  5.69it/s, loss=96, v_num=140, train_loss_step=110.0, val_loss=1.82e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 7/7 [00:01<00:00,  4.69it/s, loss=96, v_num=140, train_loss_step=110.0, val_loss=1.82e+3, train_loss_\u001b[A\n",
      "Epoch 28:  86%|▊| 6/7 [00:00<00:00,  6.07it/s, loss=94.1, v_num=140, train_loss_step=81.10, val_loss=1.82e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 7/7 [00:01<00:00,  4.96it/s, loss=94.1, v_num=140, train_loss_step=81.10, val_loss=1.82e+3, train_los\u001b[A\n",
      "Epoch 29:  86%|▊| 6/7 [00:00<00:00,  6.10it/s, loss=95.1, v_num=140, train_loss_step=89.10, val_loss=1.82e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 7/7 [00:01<00:00,  5.00it/s, loss=95.1, v_num=140, train_loss_step=89.10, val_loss=1.81e+3, train_los\u001b[A\n",
      "Epoch 30:  86%|▊| 6/7 [00:00<00:00,  6.56it/s, loss=93.1, v_num=140, train_loss_step=79.30, val_loss=1.81e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 7/7 [00:01<00:00,  5.26it/s, loss=93.1, v_num=140, train_loss_step=79.30, val_loss=1.81e+3, train_los\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:  86%|▊| 6/7 [00:01<00:00,  5.99it/s, loss=90.2, v_num=140, train_loss_step=77.50, val_loss=1.81e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 7/7 [00:01<00:00,  4.96it/s, loss=90.2, v_num=140, train_loss_step=77.50, val_loss=1.81e+3, train_los\u001b[A\n",
      "Epoch 32:  86%|▊| 6/7 [00:00<00:00,  6.26it/s, loss=89.9, v_num=140, train_loss_step=81.20, val_loss=1.81e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 7/7 [00:01<00:00,  5.11it/s, loss=89.9, v_num=140, train_loss_step=81.20, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Epoch 33:  86%|▊| 6/7 [00:00<00:00,  6.50it/s, loss=87.8, v_num=140, train_loss_step=89.90, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 7/7 [00:01<00:00,  3.64it/s, loss=87.8, v_num=140, train_loss_step=89.90, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Epoch 34:  86%|▊| 6/7 [00:01<00:00,  5.52it/s, loss=88.9, v_num=140, train_loss_step=82.70, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 7/7 [00:01<00:00,  4.60it/s, loss=88.9, v_num=140, train_loss_step=82.70, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Epoch 35:  86%|▊| 6/7 [00:00<00:00,  6.40it/s, loss=86.5, v_num=140, train_loss_step=75.30, val_loss=1.8e+3, train_loss\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 7/7 [00:01<00:00,  5.09it/s, loss=86.5, v_num=140, train_loss_step=75.30, val_loss=1.79e+3, train_los\u001b[A\n",
      "Epoch 36:  86%|▊| 6/7 [00:01<00:00,  5.83it/s, loss=86.3, v_num=140, train_loss_step=76.20, val_loss=1.79e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 7/7 [00:01<00:00,  4.83it/s, loss=86.3, v_num=140, train_loss_step=76.20, val_loss=1.79e+3, train_los\u001b[A\n",
      "Epoch 37:  86%|▊| 6/7 [00:00<00:00,  6.00it/s, loss=84.9, v_num=140, train_loss_step=109.0, val_loss=1.79e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 7/7 [00:01<00:00,  5.04it/s, loss=84.9, v_num=140, train_loss_step=109.0, val_loss=1.79e+3, train_los\u001b[A\n",
      "Epoch 38:  86%|▊| 6/7 [00:01<00:00,  5.77it/s, loss=84.8, v_num=140, train_loss_step=88.40, val_loss=1.79e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 7/7 [00:01<00:00,  4.72it/s, loss=84.8, v_num=140, train_loss_step=88.40, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 39:  86%|▊| 6/7 [00:00<00:00,  6.06it/s, loss=83.1, v_num=140, train_loss_step=84.20, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 7/7 [00:01<00:00,  4.98it/s, loss=83.1, v_num=140, train_loss_step=84.20, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 40:  86%|▊| 6/7 [00:00<00:00,  6.44it/s, loss=85.4, v_num=140, train_loss_step=88.00, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 7/7 [00:01<00:00,  5.26it/s, loss=85.4, v_num=140, train_loss_step=88.00, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 41:  86%|▊| 6/7 [00:00<00:00,  6.53it/s, loss=81.8, v_num=140, train_loss_step=69.50, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 7/7 [00:01<00:00,  5.29it/s, loss=81.8, v_num=140, train_loss_step=69.50, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 42:  86%|▊| 6/7 [00:01<00:00,  5.88it/s, loss=80.3, v_num=140, train_loss_step=65.20, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 7/7 [00:01<00:00,  4.97it/s, loss=80.3, v_num=140, train_loss_step=65.20, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 43:  86%|▊| 6/7 [00:00<00:00,  6.35it/s, loss=80.2, v_num=140, train_loss_step=77.80, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 7/7 [00:02<00:00,  3.34it/s, loss=80.2, v_num=140, train_loss_step=77.80, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 44:  86%|▊| 6/7 [00:01<00:00,  5.01it/s, loss=78.7, v_num=140, train_loss_step=66.60, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 7/7 [00:01<00:00,  4.08it/s, loss=78.7, v_num=140, train_loss_step=66.60, val_loss=1.78e+3, train_los\u001b[A\n",
      "Epoch 45:  86%|▊| 6/7 [00:01<00:00,  5.96it/s, loss=78.4, v_num=140, train_loss_step=86.70, val_loss=1.78e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 7/7 [00:01<00:00,  4.85it/s, loss=78.4, v_num=140, train_loss_step=86.70, val_loss=1.77e+3, train_los\u001b[A\n",
      "Epoch 46:  86%|▊| 6/7 [00:00<00:00,  6.31it/s, loss=79.4, v_num=140, train_loss_step=104.0, val_loss=1.77e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 7/7 [00:01<00:00,  5.06it/s, loss=79.4, v_num=140, train_loss_step=104.0, val_loss=1.77e+3, train_los\u001b[A\n",
      "Epoch 47:  86%|▊| 6/7 [00:00<00:00,  6.10it/s, loss=77.5, v_num=140, train_loss_step=91.50, val_loss=1.77e+3, train_los\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 7/7 [00:01<00:00,  4.94it/s, loss=77.5, v_num=140, train_loss_step=91.50, val_loss=1.77e+3, train_los\u001b[A\n",
      "Epoch 48:  86%|▊| 6/7 [00:01<00:00,  5.38it/s, loss=79, v_num=140, train_loss_step=79.40, val_loss=1.77e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 7/7 [00:01<00:00,  4.59it/s, loss=79, v_num=140, train_loss_step=79.40, val_loss=1.77e+3, train_loss_\u001b[A\n",
      "Epoch 49:  86%|▊| 6/7 [00:01<00:00,  5.70it/s, loss=79, v_num=140, train_loss_step=77.00, val_loss=1.77e+3, train_loss_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 7/7 [00:01<00:00,  4.79it/s, loss=79, v_num=140, train_loss_step=77.00, val_loss=1.77e+3, train_loss_\u001b[A\n",
      "Epoch 50:  14%|▏| 1/7 [00:00<00:01,  3.67it/s, loss=78.6, v_num=140, train_loss_step=74.60, val_loss=1.77e+3, train_los\u001b[A"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db23527",
   "metadata": {},
   "source": [
    "## trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3dc4976c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=17-v2.ckpt'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "# 1\n",
    "# 3 不小心删了\n",
    "# 7 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=17-v2.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3b9746b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path ='D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=13.ckpt'\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b00cde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4961.4565, 5102.7539, 5149.1890, 2539.7590, 4243.3608, 5048.9487,\n",
       "         5073.5513]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 趋势\n",
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "# 真实值\n",
    "trend_t=[[17590.5625, 17614.4883, 17650.7695, 17687.3535, 17721.1836, 17752.0820,\n",
    "         17781.7734]]\n",
    "x['decoder_target']\n",
    "\n",
    "# 1 [[17781.7734]]\n",
    "# 3 [[17721.1836, 17752.0820, 17781.7734]]\n",
    "# 7 [[17590.5625, 17614.4883, 17650.7695, 17687.3535, 17721.1836, 17752.0820,\n",
    "#          17781.7734]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92447166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4995.1235, 5844.5112, 5795.3169, 5857.1172, 5520.6118, 5817.8086,\n",
       "         5208.3574]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50%分位数\n",
    "trend=[[16752.2773, 16507.8223, 16709.7539, 16561.0078, 14986.1904, 15933.0547,\n",
    "    16418.3398]]\n",
    "raw_predictions[0][:, :, 3]\n",
    "\n",
    "# 1 [[18265.6191]]\n",
    "# 3 [[16270.6729, 16882.4727, 16940.2227]]\n",
    "# 7 [[16752.2773, 16507.8223, 16709.7539, 16561.0078, 14986.1904, 15933.0547,\n",
    "#          16418.3398]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "%tensorboard --logdir lightning_logs\n",
    "# 1 113\n",
    "# 3 119\n",
    "# 7 127\n",
    "# 15 \n",
    "# 30 \n",
    "# 60 \n",
    "# 90 \n",
    "# cd D:/Jupyter notebook/TFT/lightning_logs\n",
    "# tensorboard --logdir lightning_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe28fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "383e8155",
   "metadata": {},
   "source": [
    "## seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa79a633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=10-v3.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.checkpoint_callback.best_model_path\n",
    "# 1 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=67.ckpt'\n",
    "# 3  'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=14-v6.ckpt'\n",
    "# 7 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=10-v3.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b0f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path ='D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=13.ckpt'\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7185fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4961.4565, 5102.7539, 5149.1890, 2539.7590, 4243.3608, 5048.9487,\n",
       "         5073.5513]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "# 真实值\n",
    "season_t=[[6813.9805, 3752.7571, 4471.0425, 7904.8877, 2244.4563, 2100.9692,\n",
    "          230.6764]]\n",
    "x['decoder_target']\n",
    "# 1 [[230.6764]]\n",
    "# 3 [[2244.4563, 2100.9692,  230.6764]]\n",
    "# 7 [[6813.9805, 3752.7571, 4471.0425, 7904.8877, 2244.4563, 2100.9692,\n",
    "#           230.6764]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b951a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4995.1235, 5844.5112, 5795.3169, 5857.1172, 5520.6118, 5817.8086,\n",
       "         5208.3574]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50%分位数\n",
    "season=[[4113.8159, 3764.3884, 3863.7390, 3846.8425, 3964.4387, 3843.1409,\n",
    "         4141.3496]]\n",
    "raw_predictions[0][:, :, 3]\n",
    "# 1 [[3974.3286]]\n",
    "# 3 [[3939.8679, 3936.6040, 4264.2871]]\n",
    "# 7 \n",
    "# [[4113.8159, 3764.3884, 3863.7390, 3846.8425, 3964.4387, 3843.1409,\n",
    "#          4141.3496]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "%tensorboard --logdir lightning_logs\n",
    "# 1 116\n",
    "# 3 126\n",
    "# 7 131\n",
    "# 15 \n",
    "# 30 \n",
    "# 60 \n",
    "# 90 \n",
    "# cd D:/Jupyter notebook/TFT/lightning_logs\n",
    "# tensorboard --logdir lightning_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59eaa24",
   "metadata": {},
   "source": [
    "## resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a21324b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=23-v1.ckpt'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.checkpoint_callback.best_model_path\n",
    "# 1 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=6-v3.ckpt'\n",
    "# 3 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=5-v7.ckpt'\n",
    "# 7 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=4-v3.ckpt'\n",
    "# 15 'D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=23-v1.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af094d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path ='D:\\\\Jupyter notebook\\\\TFT\\\\saved_models\\\\best_model_epoch=13.ckpt'\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f873a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8493.0791,  6935.9995,  5236.9570,  4950.0137,  5191.2085,  5196.5332,\n",
       "         10220.0273,  9167.5908,  4961.4565,  5102.7539,  5149.1890,  2539.7590,\n",
       "          4243.3608,  5048.9487,  5073.5513]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 趋势\n",
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "# 真实值\n",
    "resid_t=[[ 8493.0791,  6935.9995,  5236.9570,  4950.0137,  5191.2085,  5196.5332,\n",
    "         10220.0273,  9167.5908,  4961.4565,  5102.7539,  5149.1890,  2539.7590,\n",
    "          4243.3608,  5048.9487,  5073.5513]]\n",
    "x['decoder_target']\n",
    "# 1 [[5073.5513]]\n",
    "# 3 [[4243.3608, 5048.9487, 5073.5513]]\n",
    "# 7\n",
    "# [[4961.4565, 5102.7539, 5149.1890, 2539.7590, 4243.3608, 5048.9487,\n",
    "#          5073.5513]]     \n",
    "# 15\n",
    "# [[ 8493.0791,  6935.9995,  5236.9570,  4950.0137,  5191.2085,  5196.5332,\n",
    "#          10220.0273,  9167.5908,  4961.4565,  5102.7539,  5149.1890,  2539.7590,\n",
    "#           4243.3608,  5048.9487,  5073.5513]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b0fa71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5398.4741, 5379.2378, 4713.4468, 4839.1294, 5059.2065, 5191.2876,\n",
       "         5187.0586, 5528.4873, 5407.6030, 4954.3867, 5393.2856, 5137.5254,\n",
       "         3823.7744, 4239.0381, 5347.8823]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50%分位数\n",
    "resid=[[5398.4741, 5379.2378, 4713.4468, 4839.1294, 5059.2065, 5191.2876,\n",
    "         5187.0586, 5528.4873, 5407.6030, 4954.3867, 5393.2856, 5137.5254,\n",
    "         3823.7744, 4239.0381, 5347.8823]]\n",
    "raw_predictions[0][:, :, 3]\n",
    "# 1 [[4937.7310]]\n",
    "# 3 [[4886.5132, 5101.5098, 5329.3799]]\n",
    "# 7\n",
    "# [[4995.1235, 5844.5112, 5795.3169, 5857.1172, 5520.6118, 5817.8086,\n",
    "#          5208.3574]]\n",
    "# 15\n",
    "# [[5398.4741, 5379.2378, 4713.4468, 4839.1294, 5059.2065, 5191.2876,\n",
    "#          5187.0586, 5528.4873, 5407.6030, 4954.3867, 5393.2856, 5137.5254,\n",
    "#          3823.7744, 4239.0381, 5347.8823]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "%tensorboard --logdir lightning_logs\n",
    "# 1 118\n",
    "# 3 124\n",
    "# 7 132\n",
    "# 15 \n",
    "# 30 \n",
    "# 60 \n",
    "# 90 \n",
    "# cd D:/Jupyter notebook/TFT/lightning_logs\n",
    "# tensorboard --logdir lightning_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9b7cc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tourist: [[16861.2167 17116.7219 17368.8098 17264.9675 15471.2409 16594.0042\n",
      "  16768.0468]]\n",
      "tourist_t: [[20365.9995 17469.9993 18271.001  19132.0002 15209.0007 15901.9999\n",
      "  14086.0011]]\n",
      "MAE: 1466.2248999999986\n",
      "RMSE: 1868.883412680888\n",
      "MAPE: 0.08434876507515128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# 模型预测值\n",
    "trend = np.array(trend)\n",
    "season = np.array(season)\n",
    "resid = np.array(resid)\n",
    "\n",
    "# 真实值\n",
    "trend_t = np.array(trend_t)\n",
    "season_t = np.array(season_t)\n",
    "resid_t = np.array(resid_t)\n",
    "\n",
    "# 计算Tourist和Tourist_t\n",
    "Tourist = trend + season + resid-9000\n",
    "Tourist_t = trend_t + season_t + resid_t-9000\n",
    "Tourist\n",
    "\n",
    "# 计算MAE、RMSE和MAPE\n",
    "MAE = np.mean(np.abs(Tourist - Tourist_t))\n",
    "RMSE = np.sqrt(np.mean((Tourist - Tourist_t)**2))\n",
    "MAPE = np.mean(np.abs((Tourist - Tourist_t) / Tourist_t)) \n",
    "print(\"tourist:\", Tourist)\n",
    "print(\"tourist_t:\", Tourist_t)\n",
    "print(\"MAE:\", MAE)\n",
    "print(\"RMSE:\", RMSE)\n",
    "print(\"MAPE:\", MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852eb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# tourist: [[27177.6787]]\n",
    "# tourist_t: [[23086.0011]]\n",
    "\n",
    "# 3\n",
    "# tourist: [[16097.054  16920.5865 17533.8897]]\n",
    "# tourist_t: [[15209.0007 15901.9999 14086.0011]]\n",
    "\n",
    "# 7\n",
    "# tourist: [[16861.2167 17116.7219 17368.8098 17264.9675 15471.2409 16594.0042\n",
    "#   16768.0468]]\n",
    "# tourist_t: [[20365.9995 17469.9993 18271.001  19132.0002 15209.0007 15901.9999\n",
    "#   14086.0011]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85af6684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "# MAE: 4091.6776000000027\n",
    "# RMSE: 4091.6776000000027\n",
    "# MAPE: 0.2904\n",
    "\n",
    "# 3\n",
    "# MAE: 1784.8428333333322\n",
    "# RMSE: 2138.0749225530285\n",
    "# MAPE: 0.12240603418067215\n",
    "\n",
    "# 7\n",
    "# MAE: 1466.2248999999986\n",
    "# RMSE: 1868.883412680888\n",
    "# MAPE: 0.08434876507515128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4aa78955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16270.6729, 16882.4727, 16940.2227]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "79ef7779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3939.8679, 3936.604 , 4264.2871]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c26b0dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4886.5132, 5101.5098, 5329.3799]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db25eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
