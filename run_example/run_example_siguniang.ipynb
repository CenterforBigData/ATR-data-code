{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ea93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b3288dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tourist</th>\n",
       "      <th>pc_Siguniang</th>\n",
       "      <th>mob_Siguniang</th>\n",
       "      <th>pc_SichuanEpidemic</th>\n",
       "      <th>mob_SichuanEpidemic</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>weekday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>destination</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Seasonal</th>\n",
       "      <th>Resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>101.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>149.586843</td>\n",
       "      <td>969.914316</td>\n",
       "      <td>2481.498840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>122.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>166.207496</td>\n",
       "      <td>957.303377</td>\n",
       "      <td>2498.489127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>149.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>181.796675</td>\n",
       "      <td>990.298899</td>\n",
       "      <td>2476.904426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>850.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>196.325958</td>\n",
       "      <td>955.000971</td>\n",
       "      <td>3198.673070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>210.204584</td>\n",
       "      <td>1230.235815</td>\n",
       "      <td>3558.559601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>2021-09-08</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>526</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>1654.920633</td>\n",
       "      <td>702.491817</td>\n",
       "      <td>2487.587550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>527</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>1761.435876</td>\n",
       "      <td>780.546667</td>\n",
       "      <td>2510.017457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2021-09-10</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>528</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>1869.009378</td>\n",
       "      <td>956.961883</td>\n",
       "      <td>2519.028739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2021-09-11</td>\n",
       "      <td>3795.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>889.0</td>\n",
       "      <td>529</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>1977.625607</td>\n",
       "      <td>1672.263550</td>\n",
       "      <td>3645.110844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2021-09-12</td>\n",
       "      <td>3165.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>530</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>SiGuniang</td>\n",
       "      <td>2087.050936</td>\n",
       "      <td>1484.639438</td>\n",
       "      <td>3093.309625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  tourist  pc_Siguniang  mob_Siguniang  pc_SichuanEpidemic  \\\n",
       "0   2020-04-01    101.0         388.0          856.0               271.0   \n",
       "1   2020-04-02    122.0         445.0          873.0               243.0   \n",
       "2   2020-04-03    149.0         333.0          877.0               201.0   \n",
       "3   2020-04-04    850.0         218.0          945.0               116.0   \n",
       "4   2020-04-05   1499.0         180.0          912.0               106.0   \n",
       "..         ...      ...           ...            ...                 ...   \n",
       "525 2021-09-08   1345.0         454.0          965.0               137.0   \n",
       "526 2021-09-09   1552.0         439.0          986.0               146.0   \n",
       "527 2021-09-10   1845.0         426.0         1077.0               137.0   \n",
       "528 2021-09-11   3795.0         265.0         1206.0               146.0   \n",
       "529 2021-09-12   3165.0         268.0         1276.0               133.0   \n",
       "\n",
       "     mob_SichuanEpidemic  time_idx    weekday  year month day destination  \\\n",
       "0                  959.0         1    Tuesday  2020     4   1   SiGuniang   \n",
       "1                  933.0         2  Wednesday  2020     4   2   SiGuniang   \n",
       "2                  841.0         3   Thursday  2020     4   3   SiGuniang   \n",
       "3                  886.0         4     Friday  2020     4   4   SiGuniang   \n",
       "4                  794.0         5   Saturday  2020     4   5   SiGuniang   \n",
       "..                   ...       ...        ...   ...   ...  ..         ...   \n",
       "525                634.0       526    Tuesday  2021     9   8   SiGuniang   \n",
       "526                652.0       527  Wednesday  2021     9   9   SiGuniang   \n",
       "527                586.0       528   Thursday  2021     9  10   SiGuniang   \n",
       "528                889.0       529     Friday  2021     9  11   SiGuniang   \n",
       "529                908.0       530   Saturday  2021     9  12   SiGuniang   \n",
       "\n",
       "           Trend     Seasonal        Resid  \n",
       "0     149.586843   969.914316  2481.498840  \n",
       "1     166.207496   957.303377  2498.489127  \n",
       "2     181.796675   990.298899  2476.904426  \n",
       "3     196.325958   955.000971  3198.673070  \n",
       "4     210.204584  1230.235815  3558.559601  \n",
       "..           ...          ...          ...  \n",
       "525  1654.920633   702.491817  2487.587550  \n",
       "526  1761.435876   780.546667  2510.017457  \n",
       "527  1869.009378   956.961883  2519.028739  \n",
       "528  1977.625607  1672.263550  3645.110844  \n",
       "529  2087.050936  1484.639438  3093.309625  \n",
       "\n",
       "[530 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset from an Excel file into a pandas DataFrame.\n",
    "# The path provided should be updated to where the actual file is located.\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('.\\dataset\\Siguniang.xlsx')\n",
    "\n",
    "data[\"year\"] = data[\"year\"].astype(str)\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "data[\"tourist\"] = data[\"tourist\"].astype(\"float64\")\n",
    "data[\"pc_Siguniang\"] = data[\"pc_Siguniang\"].astype(\"float64\")\n",
    "data[\"mob_Siguniang\"] = data[\"mob_Siguniang\"].astype(\"float64\")\n",
    "data[\"pc_SichuanEpidemic\"] = data[\"pc_SichuanEpidemic\"].astype(\"float64\")\n",
    "data[\"mob_SichuanEpidemic\"] = data[\"mob_SichuanEpidemic\"].astype(\"float64\")\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Trend\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Siguniang\",\"mob_Siguniang\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Trend\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb69a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is dedicated to hyperparameter optimization using the Optuna framework\n",
    "# integrated within the pytorch-forecasting package. This step is optional due to its\n",
    "# time-consuming nature, but it is crucial for fine-tuning the model to achieve better performance.\n",
    "\n",
    "\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "# Initialize the hyperparameter optimization study which will search for the best hyperparameters\n",
    "# over a specified number of trials.\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,  # DataLoader containing the training data\n",
    "    val_dataloader,  # DataLoader containing the validation data\n",
    "    model_path=\"optuna_test\",  # Directory where the models are saved during optimization\n",
    "    n_trials=50,  # Number of trials to run\n",
    "    max_epochs=50,  # Maximum number of epochs to train the model for each trial\n",
    "    gradient_clip_val_range=(0.01, 1.0),  # Range for gradient clipping for avoiding exploding gradients\n",
    "    hidden_size_range=(8, 128),  # Range for the size of hidden layers\n",
    "    hidden_continuous_size_range=(8, 128),  # Range for the size of hidden continuous layers\n",
    "    attention_head_size_range=(1, 4),  # Range for the number of attention heads\n",
    "    learning_rate_range=(0.001, 0.1),  # Range for the learning rate\n",
    "    dropout_range=(0.1, 0.3),  # Range for dropout rates to prevent overfitting\n",
    "    trainer_kwargs=dict(limit_train_batches=30),  # Limit the number of batches for training to speed up epochs\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate when a plateau is reached\n",
    "    use_learning_rate_finder=False  # Whether to use the learning rate finder (here it is turned off)\n",
    ")\n",
    "\n",
    "# Save the results of the study to a file so that we can resume the optimization later if needed.\n",
    "# This is useful for long-running optimizations that may need to be stopped and restarted.\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# After the optimization study is complete, print out the best hyperparameters found.\n",
    "# These parameters can be used to configure the model for the final training.\n",
    "print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1c651",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679ff9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 521.1k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.0894,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0294,  # Learning rate of the model\n",
    "    hidden_size=82,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.1962,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=39,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb0cddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# Modify this section to choose between utilizing a pre-trained model or opting for the best model from the current training session.\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'trend_siguniang.ckpt'\n",
    " \n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "544ec259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1810.7145, 1920.9052, 2064.9780]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7fbdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trend_forecasting=[1810.7145, 1920.9052, 2064.9780]\n",
    "Trend_true=[1869.0094, 1977.6256, 2087.0510]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c4e09",
   "metadata": {},
   "source": [
    "## Seasonality\n",
    "#### The following code is a repetition of the 3rd to 10th code blocks from the same notebook, adjusted to predict the 'Seasonal' component instead of 'Trend'. The 'target' and 'time_varying_unknown_reals' parameters are modified accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4abc1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Seasonal\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Siguniang\",\"mob_Siguniang\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Seasonal\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a8559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9f8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 521.1k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.0894,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0294,  # Learning rate of the model\n",
    "    hidden_size=82,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.1962,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=39,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded1994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'seasonal_siguniang.ckpt'\n",
    "\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1785ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 949.7791, 1637.7009, 1487.3295]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab4240c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seasonal_forecasting=[ 949.7791, 1637.7009, 1487.3295]\n",
    "Seasonal_true=[ 956.9619, 1672.2635, 1484.6394]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4436",
   "metadata": {},
   "source": [
    "# Resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d666296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Resid\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Siguniang\",\"mob_Siguniang\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Resid\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d65ebf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0785ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 521.1k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.0894,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0294,  # Learning rate of the model\n",
    "    hidden_size=82,  # Size of the hidden state in the model\n",
    "    attention_head_size=1,  # Number of attention heads\n",
    "    dropout=0.1962,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=39,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8db5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'resid_siguniang.ckpt'\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c54465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2514.8813, 2914.3496, 3032.1069]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "755503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resid_forecasting=[2514.8813, 2914.3496, 3032.1069]\n",
    "Resid_true=[2519.0288, 3645.1108, 3093.3096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1321b9c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e1fe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tourist: [1775.3748999999998, 2972.9557000000004, 3084.4143999999997]\n",
      "tourist_t: [1845.0001000000002, 3794.9999000000007, 3165.0]\n",
      "MAE: 324.0850000000003\n",
      "RMSE: 478.57370527991486\n",
      "MAPE: 9.327038728152441\n"
     ]
    }
   ],
   "source": [
    "# Combine the forecasted 'Trend', 'Seasonal', and 'Resid' components to obtain the forecasted 'tourist' numbers.\n",
    "# The comprehension list sums the corresponding values from the three forecasts for each point in time.\n",
    "\n",
    "import numpy as np\n",
    "tourist = [sum(x) for x in zip(Trend_forecasting, Seasonal_forecasting, Resid_forecasting)]\n",
    "\n",
    "# Similarly, combine the true 'Trend', 'Seasonal', and 'Resid' components to obtain the true 'tourist' numbers.\n",
    "# This is typically used for validation or testing purposes to compare against the forecasted values.\n",
    "tourist_t = [sum(x) for x in zip(Trend_true, Seasonal_true, Resid_true)]\n",
    "\n",
    "# \"After decomposing the dataset using RobustSTL, it is possible for some sequences to have negative values.\n",
    "# To facilitate the training of the model, we adjusted the decomposed columns to ensure that most of the sequences are greater than zero.\n",
    "# This adjustment involved increasing the sum of the decomposed sequences by 3500\n",
    "# Therefore, to accurately reflect the original data scale in the final prediction results, we subtract 3500 from the total forecasted values.\n",
    "tourist = [x - 3500 for x in tourist]\n",
    "tourist_t = [x - 3500 for x in tourist_t]\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) to evaluate the accuracy of the forecasts.\n",
    "# These metrics provide a quantitative measure of the model's predictive performance.\n",
    "mae = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)))\n",
    "rmse = np.sqrt(np.mean(np.square(np.array(tourist_t) - np.array(tourist))))\n",
    "mape = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)) / np.array(tourist_t)) * 100\n",
    "\n",
    "# Output the forecasted and true 'tourist' numbers, as well as the calculated error metrics.\n",
    "print(\"tourist:\", tourist)\n",
    "print(\"tourist_t:\", tourist_t)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd97e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
