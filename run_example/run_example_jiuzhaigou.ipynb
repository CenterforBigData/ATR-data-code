{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ea93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss,MAE,MAPE,RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3288dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tourist</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>pc_Jiuzhaigou</th>\n",
       "      <th>mob_Jiuzhaigou</th>\n",
       "      <th>pc_SichuanEpidemic</th>\n",
       "      <th>mob_SichuanEpidemic</th>\n",
       "      <th>weekday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>destination</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Seasonal</th>\n",
       "      <th>Resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>208.0</td>\n",
       "      <td>1</td>\n",
       "      <td>388.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>311.003039</td>\n",
       "      <td>1931.261309</td>\n",
       "      <td>3965.735652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2</td>\n",
       "      <td>445.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>341.195455</td>\n",
       "      <td>1946.020432</td>\n",
       "      <td>4007.784113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>311.0</td>\n",
       "      <td>3</td>\n",
       "      <td>333.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>841.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>372.267806</td>\n",
       "      <td>1847.161177</td>\n",
       "      <td>4091.571017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-04</td>\n",
       "      <td>528.0</td>\n",
       "      <td>4</td>\n",
       "      <td>218.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>404.858892</td>\n",
       "      <td>2127.398485</td>\n",
       "      <td>3995.742622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>439.053772</td>\n",
       "      <td>2039.800262</td>\n",
       "      <td>5539.145966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>2021-09-08</td>\n",
       "      <td>3728.0</td>\n",
       "      <td>526</td>\n",
       "      <td>454.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>3912.603702</td>\n",
       "      <td>1840.125736</td>\n",
       "      <td>3975.270562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2021-09-09</td>\n",
       "      <td>4295.0</td>\n",
       "      <td>527</td>\n",
       "      <td>439.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>4435.783450</td>\n",
       "      <td>2100.821748</td>\n",
       "      <td>3758.394802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2021-09-10</td>\n",
       "      <td>5056.0</td>\n",
       "      <td>528</td>\n",
       "      <td>426.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>4964.977175</td>\n",
       "      <td>2096.338414</td>\n",
       "      <td>3994.684411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2021-09-11</td>\n",
       "      <td>5754.0</td>\n",
       "      <td>529</td>\n",
       "      <td>265.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>889.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>5500.962899</td>\n",
       "      <td>2200.653459</td>\n",
       "      <td>4052.383642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2021-09-12</td>\n",
       "      <td>6377.0</td>\n",
       "      <td>530</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>JiuZhaigou</td>\n",
       "      <td>6044.186441</td>\n",
       "      <td>2079.948426</td>\n",
       "      <td>4252.865133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  tourist  time_idx  pc_Jiuzhaigou  mob_Jiuzhaigou  \\\n",
       "0    2020-04-01    208.0         1          388.0           856.0   \n",
       "1    2020-04-02    295.0         2          445.0           873.0   \n",
       "2    2020-04-03    311.0         3          333.0           877.0   \n",
       "3    2020-04-04    528.0         4          218.0           945.0   \n",
       "4    2020-04-05   2018.0         5          180.0           912.0   \n",
       "..          ...      ...       ...            ...             ...   \n",
       "525  2021-09-08   3728.0       526          454.0           965.0   \n",
       "526  2021-09-09   4295.0       527          439.0           986.0   \n",
       "527  2021-09-10   5056.0       528          426.0          1077.0   \n",
       "528  2021-09-11   5754.0       529          265.0          1206.0   \n",
       "529  2021-09-12   6377.0       530          268.0          1276.0   \n",
       "\n",
       "     pc_SichuanEpidemic  mob_SichuanEpidemic    weekday  year month day  \\\n",
       "0                 271.0                959.0    Tuesday  2020     4   1   \n",
       "1                 243.0                933.0  Wednesday  2020     4   2   \n",
       "2                 201.0                841.0   Thursday  2020     4   3   \n",
       "3                 116.0                886.0     Friday  2020     4   4   \n",
       "4                 106.0                794.0   Saturday  2020     4   5   \n",
       "..                  ...                  ...        ...   ...   ...  ..   \n",
       "525               137.0                634.0    Tuesday  2021     9   8   \n",
       "526               146.0                652.0  Wednesday  2021     9   9   \n",
       "527               137.0                586.0   Thursday  2021     9  10   \n",
       "528               146.0                889.0     Friday  2021     9  11   \n",
       "529               133.0                908.0   Saturday  2021     9  12   \n",
       "\n",
       "    destination        Trend     Seasonal        Resid  \n",
       "0    JiuZhaigou   311.003039  1931.261309  3965.735652  \n",
       "1    JiuZhaigou   341.195455  1946.020432  4007.784113  \n",
       "2    JiuZhaigou   372.267806  1847.161177  4091.571017  \n",
       "3    JiuZhaigou   404.858892  2127.398485  3995.742622  \n",
       "4    JiuZhaigou   439.053772  2039.800262  5539.145966  \n",
       "..          ...          ...          ...          ...  \n",
       "525  JiuZhaigou  3912.603702  1840.125736  3975.270562  \n",
       "526  JiuZhaigou  4435.783450  2100.821748  3758.394802  \n",
       "527  JiuZhaigou  4964.977175  2096.338414  3994.684411  \n",
       "528  JiuZhaigou  5500.962899  2200.653459  4052.383642  \n",
       "529  JiuZhaigou  6044.186441  2079.948426  4252.865133  \n",
       "\n",
       "[530 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset from an Excel file into a pandas DataFrame.\n",
    "# The path provided should be updated to where the actual file is located.\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('.\\dataset\\Jiuzhaigou.xlsx')\n",
    "\n",
    "data[\"year\"] = data[\"year\"].astype(str)\n",
    "data[\"day\"] = data[\"day\"].astype(str)\n",
    "data[\"tourist\"] = data[\"tourist\"].astype(\"float64\")\n",
    "data[\"pc_Jiuzhaigou\"] = data[\"pc_Jiuzhaigou\"].astype(\"float64\")\n",
    "data[\"mob_Jiuzhaigou\"] = data[\"mob_Jiuzhaigou\"].astype(\"float64\")\n",
    "data[\"pc_SichuanEpidemic\"] = data[\"pc_SichuanEpidemic\"].astype(\"float64\")\n",
    "data[\"mob_SichuanEpidemic\"] = data[\"mob_SichuanEpidemic\"].astype(\"float64\")\n",
    "data[\"month\"] = data[\"month\"].astype(str)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "    #data.iloc[3925:training_cutoff,:],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Trend\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Jiuzhaigou\",\"mob_Jiuzhaigou\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Trend\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    \n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb69a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is dedicated to hyperparameter optimization using the Optuna framework\n",
    "# integrated within the pytorch-forecasting package. This step is optional due to its\n",
    "# time-consuming nature, but it is crucial for fine-tuning the model to achieve better performance.\n",
    "\n",
    "\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "\n",
    "# Initialize the hyperparameter optimization study which will search for the best hyperparameters\n",
    "# over a specified number of trials.\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,  # DataLoader containing the training data\n",
    "    val_dataloader,  # DataLoader containing the validation data\n",
    "    model_path=\"optuna_test\",  # Directory where the models are saved during optimization\n",
    "    n_trials=50,  # Number of trials to run\n",
    "    max_epochs=50,  # Maximum number of epochs to train the model for each trial\n",
    "    gradient_clip_val_range=(0.01, 1.0),  # Range for gradient clipping for avoiding exploding gradients\n",
    "    hidden_size_range=(8, 128),  # Range for the size of hidden layers\n",
    "    hidden_continuous_size_range=(8, 128),  # Range for the size of hidden continuous layers\n",
    "    attention_head_size_range=(1, 4),  # Range for the number of attention heads\n",
    "    learning_rate_range=(0.001, 0.1),  # Range for the learning rate\n",
    "    dropout_range=(0.1, 0.3),  # Range for dropout rates to prevent overfitting\n",
    "    trainer_kwargs=dict(limit_train_batches=30),  # Limit the number of batches for training to speed up epochs\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate when a plateau is reached\n",
    "    use_learning_rate_finder=False  # Whether to use the learning rate finder (here it is turned off)\n",
    ")\n",
    "\n",
    "# Save the results of the study to a file so that we can resume the optimization later if needed.\n",
    "# This is useful for long-running optimizations that may need to be stopped and restarted.\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# After the optimization study is complete, print out the best hyperparameters found.\n",
    "# These parameters can be used to configure the model for the final training.\n",
    "print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1c651",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679ff9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 415.5k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.7394,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0554,  # Learning rate of the model\n",
    "    hidden_size=69,  # Size of the hidden state in the model\n",
    "    attention_head_size=3,  # Number of attention heads\n",
    "    dropout=0.1687,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=43,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0cddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# Modify this section to choose between utilizing a pre-trained model or opting for the best model from the current training session.\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'trend_jiuzhaigou.ckpt'\n",
    " \n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544ec259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5161.9409, 6050.7100, 6278.2407]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fbdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trend_forecasting=[5161.9409, 6050.7100, 6278.2407]\n",
    "Trend_true=[4964.9771, 5500.9629, 6044.1865]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c4e09",
   "metadata": {},
   "source": [
    "## Seasonality\n",
    "#### The following code is a repetition of the 3rd to 10th code blocks from the same notebook, adjusted to predict the 'Seasonal' component instead of 'Trend'. The 'target' and 'time_varying_unknown_reals' parameters are modified accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abc1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Seasonal\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Jiuzhaigou\",\"mob_Jiuzhaigou\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Seasonal\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    \n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9f8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 415.5k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.7394,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0554,  # Learning rate of the model\n",
    "    hidden_size=69,  # Size of the hidden state in the model\n",
    "    attention_head_size=3,  # Number of attention heads\n",
    "    dropout=0.1687,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=43,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded1994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'seasonal_jiuzhaigou.ckpt'\n",
    "\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1785ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2032.2938, 2018.1403, 1951.7882]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4240c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seasonal_forecasting=[2032.2938, 2018.1403, 1951.7882]\n",
    "Seasonal_true=[2096.3384, 2200.6536, 2079.9485]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4436",
   "metadata": {},
   "source": [
    "# Resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d666296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "\n",
    "# Define the maximum prediction length and encoder length\n",
    "max_prediction_length = 3  # The number of time steps the model is predicting into the future\n",
    "max_encoder_length = 30  # The number of past time steps the model is using to make predictions\n",
    "\n",
    "# Initialize a TimeSeriesDataSet object, which structures the data for the Temporal Fusion Transformer model.\n",
    "# It includes various parameters to configure the dataset for time series forecasting.\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= 440],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Resid\",\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_categoricals=[\"month\",\"weekday\",\"day\"],\n",
    "    time_varying_known_reals=[\"time_idx\",\"pc_Jiuzhaigou\",\"mob_Jiuzhaigou\",\"pc_SichuanEpidemic\",\"mob_SichuanEpidemic\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Resid\",\n",
    "        \n",
    "    ],\n",
    "    group_ids=['destination'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=['destination'], transformation=\"softplus\"),\n",
    "    \n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    "\n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65ebf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously created TimeSeriesDataSet for training, we create a validation dataset.\n",
    "# The predict=True flag indicates that the validation dataset should be set up for prediction tasks,\n",
    "# specifically, it should include the last 'max_prediction_length' points for each time series.\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,  # Use the same configurations as the training dataset\n",
    "    data,  # Source data\n",
    "    predict=True,  # Indicates the dataset is for prediction\n",
    "    stop_randomization=True  # Disables randomization when creating batches of data\n",
    ")\n",
    "\n",
    "# Create PyTorch DataLoaders for the model. These will be used to efficiently load data in batches during training and validation.\n",
    "batch_size = 128  # The batch size defines how many samples per batch to load. It is set to 128 and should be adjusted according to the available memory.\n",
    "\n",
    "# Check if a GPU is available and set PyTorch to use the GPU if possible. Otherwise, it will default to using the CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU\n",
    "else:\n",
    "    device = \"cpu\"  # Use CPU\n",
    "\n",
    "# Convert the training and validation datasets to PyTorch DataLoaders.\n",
    "# The DataLoaders are moved to the specified device (either GPU or CPU).\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)  # DataLoader for training\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*10, num_workers=0)  # DataLoader for validation, with a larger batch size to speed up evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0785ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\myenv1\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 415.5k\n"
     ]
    }
   ],
   "source": [
    "# This block configures the network and the training process. It is specifically set up to train a model\n",
    "# to predict the 'Trend' component of the dataset. Subsequent models will be trained similarly to predict\n",
    "# the 'Seasonal' and 'Resid' components.from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup checkpoints to save the model during training, specifically we save:\n",
    "# - The last model weights\n",
    "# - The model with the lowest validation loss\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss for checkpointing\n",
    "    mode=\"min\",  # Mode 'min' saves the model when the monitored metric (val_loss) is minimized\n",
    "    save_last=True,  # Save the last model state at the end of training\n",
    "    save_top_k=1,  # Save only the top 1 model with the lowest val_loss\n",
    "    filename=\"best_model_{epoch}\",  # Custom filename for the checkpoints\n",
    "    dirpath=\"saved_models\"  # Directory to save model checkpoints\n",
    ")\n",
    "\n",
    "# Early stopping callback to stop training early if validation loss has not improved\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=False,\n",
    "    mode=\"min\"  # Mode 'min' will stop when the quantity monitored has stopped decreasing\n",
    ")\n",
    "\n",
    "# Learning rate monitor to log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "# TensorBoard logger for visualization\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "# Initialize the Trainer with configurations like max epochs, GPU usage, gradient clipping\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Number of maximum epochs to train the model\n",
    "    gpus=1,  # Number of GPUs to use for training\n",
    "    enable_model_summary=True,  # Enables the printing of a model summary before training\n",
    "    gradient_clip_val=0.7394,  # Gradient clipping value for avoiding exploding gradients\n",
    "    limit_train_batches=30,  # Limiting the number of batches per training epoch for faster training\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],  # List of callbacks to be used during training\n",
    "    logger=logger,  # Logger to be used for training process\n",
    ")\n",
    "\n",
    "# Initialize the Temporal Fusion Transformer model with specific hyperparameters\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,  # TimeSeriesDataSet created previously\n",
    "    learning_rate=0.0554,  # Learning rate of the model\n",
    "    hidden_size=69,  # Size of the hidden state in the model\n",
    "    attention_head_size=3,  # Number of attention heads\n",
    "    dropout=0.1687,  # Dropout rate for regularization\n",
    "    hidden_continuous_size=43,  # Size of the continuous hidden state\n",
    "    output_size=7,  # Number of outputs of the model (for quantile loss)\n",
    "    loss=QuantileLoss(),  # Type of loss function to use (quantile loss for probabilistic forecasting)\n",
    "    log_interval=10,  # Interval for logging the learning rate\n",
    "    reduce_on_plateau_patience=4,  # Patience for reducing the learning rate on plateau\n",
    ")\n",
    "\n",
    "# Print the number of parameters in the network to ensure model complexity is manageable\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the Temporal Fusion Transformer model and the PyTorch Lightning trainer are configured,\n",
    "# we can start the training process. The 'fit' method will train the model on the data provided by\n",
    "# the training DataLoader and evaluate it on the validation DataLoader.\n",
    "\n",
    "trainer.fit(\n",
    "    tft,  # The initialized Temporal Fusion Transformer model\n",
    "    train_dataloaders=train_dataloader,  # DataLoader providing the training data batch by batch\n",
    "    val_dataloaders=val_dataloader,   # DataLoader providing the validation data\n",
    ")\n",
    "\n",
    "# During the training process, the model's performance is evaluated on the validation set at the end\n",
    "# of each epoch. The best model according to the validation loss will be saved due to the ModelCheckpoint\n",
    "# callback configured earlier. Early stopping is also in place to prevent overfitting if the validation loss\n",
    "# doesn't improve for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8db5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training process is complete, we can retrieve the path to the best model's checkpoint.\n",
    "# This model checkpoint will have the lowest validation loss observed during training due to the\n",
    "# configuration of the ModelCheckpoint callback.\n",
    "\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path  # Path to the best model's checkpoint\n",
    "best_model_path = 'resid_jiuzhaigou.ckpt'\n",
    "\n",
    "# Using the best model's checkpoint, we load the trained Temporal Fusion Transformer model.\n",
    "# This model is ready for making predictions or can be used for further analysis.\n",
    "\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# The 'best_tft' object now contains the best performing model as per the validation set,\n",
    "# and it can be used to make predictions on new data or evaluate its performance on a test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5c54465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4136.9683, 4156.5464, 4173.4351]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader,mode=\"raw\", return_x=True)\n",
    "\n",
    "raw_predictions[0][:, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "755503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resid_forecasting=[4136.9683, 4156.5464, 4173.4351]\n",
    "Resid_true=[3994.6843, 4052.3835, 4252.8652]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1321b9c",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e1fe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tourist: [5331.2029999999995, 6225.396700000001, 6403.464]\n",
      "tourist_t: [5055.999800000001, 5754.0, 6377.0002]\n",
      "MAE: 257.6878999999996\n",
      "RMSE: 315.5164986555484\n",
      "MAPE: 4.683531350635875\n"
     ]
    }
   ],
   "source": [
    "# Combine the forecasted 'Trend', 'Seasonal', and 'Resid' components to obtain the forecasted 'tourist' numbers.\n",
    "# The comprehension list sums the corresponding values from the three forecasts for each point in time.\n",
    "\n",
    "import numpy as np\n",
    "tourist = [sum(x) for x in zip(Trend_forecasting, Seasonal_forecasting, Resid_forecasting)]\n",
    "\n",
    "# Similarly, combine the true 'Trend', 'Seasonal', and 'Resid' components to obtain the true 'tourist' numbers.\n",
    "# This is typically used for validation or testing purposes to compare against the forecasted values.\n",
    "tourist_t = [sum(x) for x in zip(Trend_true, Seasonal_true, Resid_true)]\n",
    "\n",
    "# \"After decomposing the dataset using RobustSTL, it is possible for some sequences to have negative values.\n",
    "# To facilitate the training of the model, we adjusted the decomposed columns to ensure that most of the sequences are greater than zero.\n",
    "# This adjustment involved increasing the sum of the decomposed sequences by 6000\n",
    "# Therefore, to accurately reflect the original data scale in the final prediction results, we subtract 6000 from the total forecasted values.\n",
    "tourist = [x - 6000 for x in tourist]\n",
    "tourist_t = [x - 6000 for x in tourist_t]\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) to evaluate the accuracy of the forecasts.\n",
    "# These metrics provide a quantitative measure of the model's predictive performance.\n",
    "mae = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)))\n",
    "rmse = np.sqrt(np.mean(np.square(np.array(tourist_t) - np.array(tourist))))\n",
    "mape = np.mean(np.abs(np.array(tourist_t) - np.array(tourist)) / np.array(tourist_t)) * 100\n",
    "\n",
    "# Output the forecasted and true 'tourist' numbers, as well as the calculated error metrics.\n",
    "print(\"tourist:\", tourist)\n",
    "print(\"tourist_t:\", tourist_t)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd97e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
